{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Importing the libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from scipy.fft import fft\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,random_split,TensorDataset\n"
      ],
      "metadata": {
        "id": "EfmsLh-U4P3Z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset"
      ],
      "metadata": {
        "id": "01CuAdAKK-Nw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImJUEWKX0Nex"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "_h12WwQ402tM",
        "outputId": "627b77f5-d114-462c-bbb8-6e7a85f3752c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7fbd07e-27ab-4037-8aeb-572c34db6848\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f7fbd07e-27ab-4037-8aeb-572c34db6848\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"arvindsharma126\",\"key\":\"03e2df86565eb6c7ec809398ad7ff37c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "CryOgXcx1KRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IYKBKRBy1aqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_5Y8RPS0l7-",
        "outputId": "88b36ee8-5dff-4a4e-acbe-c789c95d0e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ravdess-emotional-speech-audio.zip to /content\n",
            "100% 429M/429M [00:12<00:00, 40.3MB/s]\n",
            "100% 429M/429M [00:12<00:00, 37.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ravdess-emotional-speech-audio.zip -d ravdess-emotional-speech-audio"
      ],
      "metadata": {
        "id": "8YFGe0sR1rlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the audio files to a csv file"
      ],
      "metadata": {
        "id": "E0PN8gfGLHlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/ravdess-emotional-speech-audio\"\n",
        "final = np.zeros((1440,))\n",
        "k = 0\n",
        "for i in range(1,25):\n",
        "  os.chdir(path+f\"/Actor_{i:02}\")\n",
        "  for f in os.listdir():\n",
        "    y,sr = librosa.load(f,duration=2.9)\n",
        "    final[k] = len(y)\n",
        "    k+=1\n",
        "    # y = librosa.feature.mfcc(y = y,sr = sr,n_mfcc = 10000)\n",
        "    # final = np.vstack([final,y.reshape((1,-1))])\n",
        "np.std(final)"
      ],
      "metadata": {
        "id": "m5Qf5oVVE7Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpbqIWmXQjo2",
        "outputId": "61723fdb-b4fd-455c-efba-b6778963fec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([63945., 63945., 63945., ..., 63945., 63945., 63945.])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=[\"Modality\",\"Vocal channel\",\"Emotion\",\"Emotional intensity\",\"Statement\",\"Repetition\",\"Actor\"])\n",
        "j = 0\n",
        "final = []\n",
        "audio = np.zeros((63945,))\n",
        "path = \"/content/ravdess-emotional-speech-audio\"\n",
        "for i in range(1,25):\n",
        "  os.chdir(path+f\"/Actor_{i:02}\")\n",
        "  for f in os.listdir():\n",
        "    data = f.split(\".\")[0]\n",
        "    data = data.split(\"-\")\n",
        "    final.append(data)\n",
        "    y,sr = librosa.load(f,duration=2.9)\n",
        "    audio = np.vstack([audio,y.reshape((1,-1))])\n",
        "audio = audio[1:,:]"
      ],
      "metadata": {
        "id": "ki4aibu42eUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o7qjoE0TCkx",
        "outputId": "00f08425-ef0e-42f1-8e24-70e25e3f3c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['03', '01', '05', '02', '02', '01', '01'],\n",
              " ['03', '01', '04', '02', '02', '01', '01'],\n",
              " ['03', '01', '01', '01', '02', '02', '01'],\n",
              " ['03', '01', '02', '02', '02', '01', '01'],\n",
              " ['03', '01', '04', '02', '02', '02', '01'],\n",
              " ['03', '01', '03', '02', '01', '02', '01'],\n",
              " ['03', '01', '08', '02', '01', '01', '01'],\n",
              " ['03', '01', '06', '01', '01', '01', '01'],\n",
              " ['03', '01', '03', '02', '02', '01', '01'],\n",
              " ['03', '01', '07', '02', '02', '02', '01'],\n",
              " ['03', '01', '08', '01', '02', '02', '01'],\n",
              " ['03', '01', '08', '02', '02', '01', '01'],\n",
              " ['03', '01', '07', '01', '01', '02', '01'],\n",
              " ['03', '01', '03', '02', '02', '02', '01'],\n",
              " ['03', '01', '04', '01', '01', '01', '01'],\n",
              " ['03', '01', '01', '01', '02', '01', '01'],\n",
              " ['03', '01', '05', '02', '02', '02', '01'],\n",
              " ['03', '01', '06', '01', '01', '02', '01'],\n",
              " ['03', '01', '06', '02', '01', '02', '01'],\n",
              " ['03', '01', '04', '01', '01', '02', '01'],\n",
              " ['03', '01', '07', '02', '01', '01', '01'],\n",
              " ['03', '01', '04', '01', '02', '02', '01'],\n",
              " ['03', '01', '06', '01', '02', '02', '01'],\n",
              " ['03', '01', '08', '01', '01', '01', '01'],\n",
              " ['03', '01', '04', '02', '01', '02', '01'],\n",
              " ['03', '01', '03', '01', '01', '01', '01'],\n",
              " ['03', '01', '04', '01', '02', '01', '01'],\n",
              " ['03', '01', '02', '01', '02', '01', '01'],\n",
              " ['03', '01', '05', '02', '01', '01', '01'],\n",
              " ['03', '01', '04', '02', '01', '01', '01'],\n",
              " ['03', '01', '02', '02', '02', '02', '01'],\n",
              " ['03', '01', '07', '02', '01', '02', '01'],\n",
              " ['03', '01', '02', '01', '01', '02', '01'],\n",
              " ['03', '01', '05', '02', '01', '02', '01'],\n",
              " ['03', '01', '06', '02', '01', '01', '01'],\n",
              " ['03', '01', '05', '01', '01', '01', '01'],\n",
              " ['03', '01', '08', '01', '01', '02', '01'],\n",
              " ['03', '01', '05', '01', '02', '01', '01'],\n",
              " ['03', '01', '06', '02', '02', '02', '01'],\n",
              " ['03', '01', '06', '01', '02', '01', '01'],\n",
              " ['03', '01', '07', '01', '02', '01', '01'],\n",
              " ['03', '01', '03', '01', '02', '01', '01'],\n",
              " ['03', '01', '03', '01', '02', '02', '01'],\n",
              " ['03', '01', '03', '01', '01', '02', '01'],\n",
              " ['03', '01', '02', '01', '01', '01', '01'],\n",
              " ['03', '01', '02', '02', '01', '01', '01'],\n",
              " ['03', '01', '05', '01', '02', '02', '01'],\n",
              " ['03', '01', '06', '02', '02', '01', '01'],\n",
              " ['03', '01', '08', '01', '02', '01', '01'],\n",
              " ['03', '01', '01', '01', '01', '02', '01'],\n",
              " ['03', '01', '01', '01', '01', '01', '01'],\n",
              " ['03', '01', '07', '01', '02', '02', '01'],\n",
              " ['03', '01', '08', '02', '02', '02', '01'],\n",
              " ['03', '01', '02', '02', '01', '02', '01'],\n",
              " ['03', '01', '07', '02', '02', '01', '01'],\n",
              " ['03', '01', '05', '01', '01', '02', '01'],\n",
              " ['03', '01', '03', '02', '01', '01', '01'],\n",
              " ['03', '01', '02', '01', '02', '02', '01'],\n",
              " ['03', '01', '08', '02', '01', '02', '01'],\n",
              " ['03', '01', '07', '01', '01', '01', '01'],\n",
              " ['03', '01', '07', '01', '01', '01', '02'],\n",
              " ['03', '01', '02', '01', '02', '02', '02'],\n",
              " ['03', '01', '06', '01', '02', '02', '02'],\n",
              " ['03', '01', '06', '02', '02', '01', '02'],\n",
              " ['03', '01', '06', '02', '02', '02', '02'],\n",
              " ['03', '01', '06', '01', '01', '02', '02'],\n",
              " ['03', '01', '07', '01', '02', '01', '02'],\n",
              " ['03', '01', '07', '02', '01', '01', '02'],\n",
              " ['03', '01', '02', '02', '01', '01', '02'],\n",
              " ['03', '01', '07', '01', '01', '02', '02'],\n",
              " ['03', '01', '06', '02', '01', '01', '02'],\n",
              " ['03', '01', '02', '02', '02', '01', '02'],\n",
              " ['03', '01', '08', '01', '02', '02', '02'],\n",
              " ['03', '01', '04', '01', '01', '02', '02'],\n",
              " ['03', '01', '08', '01', '01', '02', '02'],\n",
              " ['03', '01', '02', '02', '02', '02', '02'],\n",
              " ['03', '01', '01', '01', '02', '01', '02'],\n",
              " ['03', '01', '05', '02', '02', '02', '02'],\n",
              " ['03', '01', '07', '02', '01', '02', '02'],\n",
              " ['03', '01', '08', '01', '01', '01', '02'],\n",
              " ['03', '01', '03', '02', '02', '01', '02'],\n",
              " ['03', '01', '04', '01', '01', '01', '02'],\n",
              " ['03', '01', '05', '01', '02', '01', '02'],\n",
              " ['03', '01', '08', '02', '01', '01', '02'],\n",
              " ['03', '01', '04', '02', '01', '01', '02'],\n",
              " ['03', '01', '01', '01', '01', '02', '02'],\n",
              " ['03', '01', '05', '01', '02', '02', '02'],\n",
              " ['03', '01', '05', '02', '01', '01', '02'],\n",
              " ['03', '01', '04', '01', '02', '01', '02'],\n",
              " ['03', '01', '03', '01', '02', '01', '02'],\n",
              " ['03', '01', '07', '02', '02', '01', '02'],\n",
              " ['03', '01', '02', '01', '01', '01', '02'],\n",
              " ['03', '01', '07', '01', '02', '02', '02'],\n",
              " ['03', '01', '03', '01', '02', '02', '02'],\n",
              " ['03', '01', '02', '01', '02', '01', '02'],\n",
              " ['03', '01', '05', '01', '01', '02', '02'],\n",
              " ['03', '01', '05', '01', '01', '01', '02'],\n",
              " ['03', '01', '02', '02', '01', '02', '02'],\n",
              " ['03', '01', '06', '01', '02', '01', '02'],\n",
              " ['03', '01', '08', '01', '02', '01', '02'],\n",
              " ['03', '01', '02', '01', '01', '02', '02'],\n",
              " ['03', '01', '04', '02', '01', '02', '02'],\n",
              " ['03', '01', '03', '01', '01', '02', '02'],\n",
              " ['03', '01', '05', '02', '02', '01', '02'],\n",
              " ['03', '01', '08', '02', '02', '01', '02'],\n",
              " ['03', '01', '08', '02', '02', '02', '02'],\n",
              " ['03', '01', '04', '02', '02', '02', '02'],\n",
              " ['03', '01', '03', '02', '01', '02', '02'],\n",
              " ['03', '01', '01', '01', '02', '02', '02'],\n",
              " ['03', '01', '06', '02', '01', '02', '02'],\n",
              " ['03', '01', '03', '02', '01', '01', '02'],\n",
              " ['03', '01', '08', '02', '01', '02', '02'],\n",
              " ['03', '01', '05', '02', '01', '02', '02'],\n",
              " ['03', '01', '04', '02', '02', '01', '02'],\n",
              " ['03', '01', '01', '01', '01', '01', '02'],\n",
              " ['03', '01', '03', '01', '01', '01', '02'],\n",
              " ['03', '01', '06', '01', '01', '01', '02'],\n",
              " ['03', '01', '04', '01', '02', '02', '02'],\n",
              " ['03', '01', '07', '02', '02', '02', '02'],\n",
              " ['03', '01', '03', '02', '02', '02', '02'],\n",
              " ['03', '01', '07', '02', '01', '01', '03'],\n",
              " ['03', '01', '08', '02', '01', '01', '03'],\n",
              " ['03', '01', '07', '01', '02', '01', '03'],\n",
              " ['03', '01', '03', '01', '02', '01', '03'],\n",
              " ['03', '01', '02', '02', '01', '02', '03'],\n",
              " ['03', '01', '02', '02', '02', '01', '03'],\n",
              " ['03', '01', '02', '01', '01', '01', '03'],\n",
              " ['03', '01', '03', '01', '01', '01', '03'],\n",
              " ['03', '01', '03', '01', '02', '02', '03'],\n",
              " ['03', '01', '03', '02', '01', '01', '03'],\n",
              " ['03', '01', '04', '01', '02', '02', '03'],\n",
              " ['03', '01', '08', '01', '02', '01', '03'],\n",
              " ['03', '01', '03', '02', '02', '01', '03'],\n",
              " ['03', '01', '02', '02', '01', '01', '03'],\n",
              " ['03', '01', '05', '01', '01', '01', '03'],\n",
              " ['03', '01', '05', '01', '01', '02', '03'],\n",
              " ['03', '01', '06', '02', '01', '02', '03'],\n",
              " ['03', '01', '06', '01', '01', '02', '03'],\n",
              " ['03', '01', '01', '01', '02', '01', '03'],\n",
              " ['03', '01', '07', '02', '02', '01', '03'],\n",
              " ['03', '01', '06', '02', '01', '01', '03'],\n",
              " ['03', '01', '08', '01', '01', '02', '03'],\n",
              " ['03', '01', '05', '01', '02', '02', '03'],\n",
              " ['03', '01', '03', '02', '01', '02', '03'],\n",
              " ['03', '01', '02', '01', '02', '01', '03'],\n",
              " ['03', '01', '07', '02', '02', '02', '03'],\n",
              " ['03', '01', '07', '01', '02', '02', '03'],\n",
              " ['03', '01', '01', '01', '01', '01', '03'],\n",
              " ['03', '01', '01', '01', '02', '02', '03'],\n",
              " ['03', '01', '08', '02', '02', '02', '03'],\n",
              " ['03', '01', '08', '01', '01', '01', '03'],\n",
              " ['03', '01', '04', '02', '02', '02', '03'],\n",
              " ['03', '01', '08', '01', '02', '02', '03'],\n",
              " ['03', '01', '04', '02', '02', '01', '03'],\n",
              " ['03', '01', '08', '02', '01', '02', '03'],\n",
              " ['03', '01', '06', '02', '02', '01', '03'],\n",
              " ['03', '01', '05', '02', '01', '01', '03'],\n",
              " ['03', '01', '03', '02', '02', '02', '03'],\n",
              " ['03', '01', '02', '02', '02', '02', '03'],\n",
              " ['03', '01', '02', '01', '02', '02', '03'],\n",
              " ['03', '01', '05', '02', '01', '02', '03'],\n",
              " ['03', '01', '03', '01', '01', '02', '03'],\n",
              " ['03', '01', '05', '02', '02', '01', '03'],\n",
              " ['03', '01', '05', '02', '02', '02', '03'],\n",
              " ['03', '01', '04', '02', '01', '02', '03'],\n",
              " ['03', '01', '06', '01', '02', '02', '03'],\n",
              " ['03', '01', '04', '01', '01', '01', '03'],\n",
              " ['03', '01', '05', '01', '02', '01', '03'],\n",
              " ['03', '01', '06', '01', '02', '01', '03'],\n",
              " ['03', '01', '07', '02', '01', '02', '03'],\n",
              " ['03', '01', '07', '01', '01', '02', '03'],\n",
              " ['03', '01', '04', '02', '01', '01', '03'],\n",
              " ['03', '01', '01', '01', '01', '02', '03'],\n",
              " ['03', '01', '04', '01', '02', '01', '03'],\n",
              " ['03', '01', '02', '01', '01', '02', '03'],\n",
              " ['03', '01', '04', '01', '01', '02', '03'],\n",
              " ['03', '01', '07', '01', '01', '01', '03'],\n",
              " ['03', '01', '06', '01', '01', '01', '03'],\n",
              " ['03', '01', '08', '02', '02', '01', '03'],\n",
              " ['03', '01', '06', '02', '02', '02', '03'],\n",
              " ['03', '01', '06', '02', '01', '01', '04'],\n",
              " ['03', '01', '03', '01', '01', '02', '04'],\n",
              " ['03', '01', '07', '02', '01', '02', '04'],\n",
              " ['03', '01', '06', '01', '02', '01', '04'],\n",
              " ['03', '01', '05', '01', '02', '02', '04'],\n",
              " ['03', '01', '07', '01', '02', '02', '04'],\n",
              " ['03', '01', '08', '01', '01', '01', '04'],\n",
              " ['03', '01', '07', '02', '01', '01', '04'],\n",
              " ['03', '01', '08', '01', '01', '02', '04'],\n",
              " ['03', '01', '04', '02', '01', '02', '04'],\n",
              " ['03', '01', '06', '02', '02', '01', '04'],\n",
              " ['03', '01', '05', '02', '01', '01', '04'],\n",
              " ['03', '01', '08', '01', '02', '02', '04'],\n",
              " ['03', '01', '04', '01', '02', '01', '04'],\n",
              " ['03', '01', '06', '02', '02', '02', '04'],\n",
              " ['03', '01', '05', '02', '01', '02', '04'],\n",
              " ['03', '01', '06', '01', '01', '01', '04'],\n",
              " ['03', '01', '06', '01', '01', '02', '04'],\n",
              " ['03', '01', '04', '01', '02', '02', '04'],\n",
              " ['03', '01', '08', '02', '02', '01', '04'],\n",
              " ['03', '01', '03', '01', '02', '01', '04'],\n",
              " ['03', '01', '03', '01', '01', '01', '04'],\n",
              " ['03', '01', '06', '01', '02', '02', '04'],\n",
              " ['03', '01', '02', '02', '02', '01', '04'],\n",
              " ['03', '01', '08', '02', '01', '01', '04'],\n",
              " ['03', '01', '05', '02', '02', '01', '04'],\n",
              " ['03', '01', '01', '01', '02', '02', '04'],\n",
              " ['03', '01', '04', '02', '02', '02', '04'],\n",
              " ['03', '01', '02', '01', '01', '02', '04'],\n",
              " ['03', '01', '02', '01', '01', '01', '04'],\n",
              " ['03', '01', '02', '02', '02', '02', '04'],\n",
              " ['03', '01', '05', '01', '01', '01', '04'],\n",
              " ['03', '01', '07', '02', '02', '02', '04'],\n",
              " ['03', '01', '03', '01', '02', '02', '04'],\n",
              " ['03', '01', '05', '01', '01', '02', '04'],\n",
              " ['03', '01', '08', '01', '02', '01', '04'],\n",
              " ['03', '01', '02', '01', '02', '01', '04'],\n",
              " ['03', '01', '07', '02', '02', '01', '04'],\n",
              " ['03', '01', '01', '01', '02', '01', '04'],\n",
              " ['03', '01', '07', '01', '02', '01', '04'],\n",
              " ['03', '01', '04', '01', '01', '02', '04'],\n",
              " ['03', '01', '08', '02', '02', '02', '04'],\n",
              " ['03', '01', '05', '02', '02', '02', '04'],\n",
              " ['03', '01', '03', '02', '01', '02', '04'],\n",
              " ['03', '01', '02', '02', '01', '01', '04'],\n",
              " ['03', '01', '02', '01', '02', '02', '04'],\n",
              " ['03', '01', '01', '01', '01', '02', '04'],\n",
              " ['03', '01', '07', '01', '01', '01', '04'],\n",
              " ['03', '01', '04', '02', '02', '01', '04'],\n",
              " ['03', '01', '03', '02', '01', '01', '04'],\n",
              " ['03', '01', '04', '02', '01', '01', '04'],\n",
              " ['03', '01', '03', '02', '02', '02', '04'],\n",
              " ['03', '01', '03', '02', '02', '01', '04'],\n",
              " ['03', '01', '01', '01', '01', '01', '04'],\n",
              " ['03', '01', '04', '01', '01', '01', '04'],\n",
              " ['03', '01', '07', '01', '01', '02', '04'],\n",
              " ['03', '01', '02', '02', '01', '02', '04'],\n",
              " ['03', '01', '06', '02', '01', '02', '04'],\n",
              " ['03', '01', '08', '02', '01', '02', '04'],\n",
              " ['03', '01', '05', '01', '02', '01', '04'],\n",
              " ['03', '01', '02', '01', '01', '01', '05'],\n",
              " ['03', '01', '07', '01', '02', '01', '05'],\n",
              " ['03', '01', '07', '01', '01', '02', '05'],\n",
              " ['03', '01', '08', '01', '02', '02', '05'],\n",
              " ['03', '01', '04', '02', '02', '01', '05'],\n",
              " ['03', '01', '03', '02', '02', '01', '05'],\n",
              " ['03', '01', '05', '01', '01', '01', '05'],\n",
              " ['03', '01', '05', '01', '02', '02', '05'],\n",
              " ['03', '01', '08', '01', '02', '01', '05'],\n",
              " ['03', '01', '08', '02', '02', '01', '05'],\n",
              " ['03', '01', '05', '02', '02', '01', '05'],\n",
              " ['03', '01', '08', '02', '02', '02', '05'],\n",
              " ['03', '01', '02', '01', '01', '02', '05'],\n",
              " ['03', '01', '02', '02', '02', '02', '05'],\n",
              " ['03', '01', '04', '01', '01', '01', '05'],\n",
              " ['03', '01', '04', '01', '02', '01', '05'],\n",
              " ['03', '01', '05', '01', '01', '02', '05'],\n",
              " ['03', '01', '03', '02', '01', '02', '05'],\n",
              " ['03', '01', '03', '01', '02', '01', '05'],\n",
              " ['03', '01', '06', '01', '01', '02', '05'],\n",
              " ['03', '01', '05', '02', '02', '02', '05'],\n",
              " ['03', '01', '07', '02', '01', '02', '05'],\n",
              " ['03', '01', '08', '01', '01', '01', '05'],\n",
              " ['03', '01', '04', '02', '01', '01', '05'],\n",
              " ['03', '01', '07', '02', '02', '02', '05'],\n",
              " ['03', '01', '05', '02', '01', '02', '05'],\n",
              " ['03', '01', '03', '02', '02', '02', '05'],\n",
              " ['03', '01', '07', '01', '01', '01', '05'],\n",
              " ['03', '01', '07', '01', '02', '02', '05'],\n",
              " ['03', '01', '02', '02', '01', '02', '05'],\n",
              " ['03', '01', '01', '01', '02', '01', '05'],\n",
              " ['03', '01', '06', '02', '01', '01', '05'],\n",
              " ['03', '01', '01', '01', '02', '02', '05'],\n",
              " ['03', '01', '01', '01', '01', '01', '05'],\n",
              " ['03', '01', '05', '01', '02', '01', '05'],\n",
              " ['03', '01', '04', '02', '01', '02', '05'],\n",
              " ['03', '01', '02', '01', '02', '02', '05'],\n",
              " ['03', '01', '06', '01', '01', '01', '05'],\n",
              " ['03', '01', '04', '01', '01', '02', '05'],\n",
              " ['03', '01', '06', '02', '01', '02', '05'],\n",
              " ['03', '01', '08', '01', '01', '02', '05'],\n",
              " ['03', '01', '01', '01', '01', '02', '05'],\n",
              " ['03', '01', '04', '01', '02', '02', '05'],\n",
              " ['03', '01', '06', '01', '02', '01', '05'],\n",
              " ['03', '01', '07', '02', '02', '01', '05'],\n",
              " ['03', '01', '08', '02', '01', '02', '05'],\n",
              " ['03', '01', '05', '02', '01', '01', '05'],\n",
              " ['03', '01', '02', '02', '01', '01', '05'],\n",
              " ['03', '01', '04', '02', '02', '02', '05'],\n",
              " ['03', '01', '03', '02', '01', '01', '05'],\n",
              " ['03', '01', '06', '02', '02', '02', '05'],\n",
              " ['03', '01', '07', '02', '01', '01', '05'],\n",
              " ['03', '01', '03', '01', '01', '01', '05'],\n",
              " ['03', '01', '02', '02', '02', '01', '05'],\n",
              " ['03', '01', '08', '02', '01', '01', '05'],\n",
              " ['03', '01', '06', '02', '02', '01', '05'],\n",
              " ['03', '01', '02', '01', '02', '01', '05'],\n",
              " ['03', '01', '03', '01', '02', '02', '05'],\n",
              " ['03', '01', '06', '01', '02', '02', '05'],\n",
              " ['03', '01', '03', '01', '01', '02', '05'],\n",
              " ['03', '01', '02', '01', '01', '02', '06'],\n",
              " ['03', '01', '04', '02', '01', '02', '06'],\n",
              " ['03', '01', '03', '01', '02', '01', '06'],\n",
              " ['03', '01', '01', '01', '02', '01', '06'],\n",
              " ['03', '01', '06', '01', '02', '02', '06'],\n",
              " ['03', '01', '03', '01', '02', '02', '06'],\n",
              " ['03', '01', '02', '02', '02', '02', '06'],\n",
              " ['03', '01', '03', '02', '01', '02', '06'],\n",
              " ['03', '01', '06', '01', '01', '01', '06'],\n",
              " ['03', '01', '03', '02', '02', '02', '06'],\n",
              " ['03', '01', '05', '01', '01', '01', '06'],\n",
              " ['03', '01', '07', '02', '01', '02', '06'],\n",
              " ['03', '01', '02', '02', '02', '01', '06'],\n",
              " ['03', '01', '05', '02', '02', '02', '06'],\n",
              " ['03', '01', '08', '02', '02', '01', '06'],\n",
              " ['03', '01', '02', '02', '01', '02', '06'],\n",
              " ['03', '01', '05', '02', '01', '01', '06'],\n",
              " ['03', '01', '03', '02', '01', '01', '06'],\n",
              " ['03', '01', '04', '01', '02', '01', '06'],\n",
              " ['03', '01', '06', '02', '01', '02', '06'],\n",
              " ['03', '01', '08', '02', '01', '01', '06'],\n",
              " ['03', '01', '02', '02', '01', '01', '06'],\n",
              " ['03', '01', '06', '02', '01', '01', '06'],\n",
              " ['03', '01', '02', '01', '01', '01', '06'],\n",
              " ['03', '01', '05', '02', '02', '01', '06'],\n",
              " ['03', '01', '03', '01', '01', '01', '06'],\n",
              " ['03', '01', '07', '02', '02', '02', '06'],\n",
              " ['03', '01', '02', '01', '02', '02', '06'],\n",
              " ['03', '01', '03', '01', '01', '02', '06'],\n",
              " ['03', '01', '04', '02', '02', '02', '06'],\n",
              " ['03', '01', '07', '02', '01', '01', '06'],\n",
              " ['03', '01', '07', '01', '01', '01', '06'],\n",
              " ['03', '01', '04', '01', '02', '02', '06'],\n",
              " ['03', '01', '02', '01', '02', '01', '06'],\n",
              " ['03', '01', '01', '01', '01', '02', '06'],\n",
              " ['03', '01', '04', '02', '01', '01', '06'],\n",
              " ['03', '01', '08', '01', '02', '01', '06'],\n",
              " ['03', '01', '05', '01', '02', '01', '06'],\n",
              " ['03', '01', '06', '01', '01', '02', '06'],\n",
              " ['03', '01', '05', '02', '01', '02', '06'],\n",
              " ['03', '01', '08', '02', '01', '02', '06'],\n",
              " ['03', '01', '06', '01', '02', '01', '06'],\n",
              " ['03', '01', '08', '01', '01', '02', '06'],\n",
              " ['03', '01', '07', '01', '02', '01', '06'],\n",
              " ['03', '01', '08', '02', '02', '02', '06'],\n",
              " ['03', '01', '07', '01', '01', '02', '06'],\n",
              " ['03', '01', '04', '02', '02', '01', '06'],\n",
              " ['03', '01', '07', '02', '02', '01', '06'],\n",
              " ['03', '01', '01', '01', '01', '01', '06'],\n",
              " ['03', '01', '05', '01', '02', '02', '06'],\n",
              " ['03', '01', '06', '02', '02', '02', '06'],\n",
              " ['03', '01', '04', '01', '01', '01', '06'],\n",
              " ['03', '01', '01', '01', '02', '02', '06'],\n",
              " ['03', '01', '03', '02', '02', '01', '06'],\n",
              " ['03', '01', '05', '01', '01', '02', '06'],\n",
              " ['03', '01', '08', '01', '01', '01', '06'],\n",
              " ['03', '01', '04', '01', '01', '02', '06'],\n",
              " ['03', '01', '06', '02', '02', '01', '06'],\n",
              " ['03', '01', '07', '01', '02', '02', '06'],\n",
              " ['03', '01', '08', '01', '02', '02', '06'],\n",
              " ['03', '01', '07', '02', '01', '02', '07'],\n",
              " ['03', '01', '05', '02', '01', '02', '07'],\n",
              " ['03', '01', '06', '02', '01', '02', '07'],\n",
              " ['03', '01', '07', '02', '02', '01', '07'],\n",
              " ['03', '01', '03', '02', '02', '02', '07'],\n",
              " ['03', '01', '07', '01', '02', '01', '07'],\n",
              " ['03', '01', '08', '02', '02', '01', '07'],\n",
              " ['03', '01', '04', '02', '01', '01', '07'],\n",
              " ['03', '01', '06', '01', '02', '01', '07'],\n",
              " ['03', '01', '06', '02', '02', '02', '07'],\n",
              " ['03', '01', '06', '01', '01', '02', '07'],\n",
              " ['03', '01', '07', '01', '01', '02', '07'],\n",
              " ['03', '01', '03', '01', '02', '02', '07'],\n",
              " ['03', '01', '08', '02', '02', '02', '07'],\n",
              " ['03', '01', '05', '02', '02', '01', '07'],\n",
              " ['03', '01', '02', '01', '02', '02', '07'],\n",
              " ['03', '01', '04', '01', '01', '02', '07'],\n",
              " ['03', '01', '01', '01', '02', '01', '07'],\n",
              " ['03', '01', '03', '01', '01', '02', '07'],\n",
              " ['03', '01', '08', '02', '01', '01', '07'],\n",
              " ['03', '01', '01', '01', '01', '01', '07'],\n",
              " ['03', '01', '05', '01', '02', '01', '07'],\n",
              " ['03', '01', '07', '01', '01', '01', '07'],\n",
              " ['03', '01', '07', '02', '01', '01', '07'],\n",
              " ['03', '01', '05', '01', '01', '01', '07'],\n",
              " ['03', '01', '03', '01', '02', '01', '07'],\n",
              " ['03', '01', '05', '01', '02', '02', '07'],\n",
              " ['03', '01', '07', '01', '02', '02', '07'],\n",
              " ['03', '01', '08', '01', '02', '01', '07'],\n",
              " ['03', '01', '08', '02', '01', '02', '07'],\n",
              " ['03', '01', '04', '01', '02', '01', '07'],\n",
              " ['03', '01', '06', '01', '01', '01', '07'],\n",
              " ['03', '01', '01', '01', '02', '02', '07'],\n",
              " ['03', '01', '08', '01', '01', '01', '07'],\n",
              " ['03', '01', '07', '02', '02', '02', '07'],\n",
              " ['03', '01', '06', '01', '02', '02', '07'],\n",
              " ['03', '01', '02', '01', '02', '01', '07'],\n",
              " ['03', '01', '02', '02', '02', '02', '07'],\n",
              " ['03', '01', '01', '01', '01', '02', '07'],\n",
              " ['03', '01', '02', '02', '01', '01', '07'],\n",
              " ['03', '01', '04', '02', '02', '01', '07'],\n",
              " ['03', '01', '04', '02', '02', '02', '07'],\n",
              " ['03', '01', '03', '02', '01', '02', '07'],\n",
              " ['03', '01', '05', '02', '02', '02', '07'],\n",
              " ['03', '01', '05', '01', '01', '02', '07'],\n",
              " ['03', '01', '03', '02', '02', '01', '07'],\n",
              " ['03', '01', '08', '01', '02', '02', '07'],\n",
              " ['03', '01', '04', '01', '02', '02', '07'],\n",
              " ['03', '01', '03', '02', '01', '01', '07'],\n",
              " ['03', '01', '04', '01', '01', '01', '07'],\n",
              " ['03', '01', '02', '01', '01', '01', '07'],\n",
              " ['03', '01', '04', '02', '01', '02', '07'],\n",
              " ['03', '01', '03', '01', '01', '01', '07'],\n",
              " ['03', '01', '06', '02', '01', '01', '07'],\n",
              " ['03', '01', '06', '02', '02', '01', '07'],\n",
              " ['03', '01', '02', '01', '01', '02', '07'],\n",
              " ['03', '01', '08', '01', '01', '02', '07'],\n",
              " ['03', '01', '02', '02', '02', '01', '07'],\n",
              " ['03', '01', '02', '02', '01', '02', '07'],\n",
              " ['03', '01', '05', '02', '01', '01', '07'],\n",
              " ['03', '01', '07', '01', '01', '02', '08'],\n",
              " ['03', '01', '04', '02', '02', '02', '08'],\n",
              " ['03', '01', '08', '02', '02', '02', '08'],\n",
              " ['03', '01', '04', '01', '02', '02', '08'],\n",
              " ['03', '01', '02', '01', '01', '02', '08'],\n",
              " ['03', '01', '03', '01', '02', '01', '08'],\n",
              " ['03', '01', '02', '02', '01', '02', '08'],\n",
              " ['03', '01', '03', '02', '02', '02', '08'],\n",
              " ['03', '01', '04', '02', '01', '01', '08'],\n",
              " ['03', '01', '06', '01', '02', '02', '08'],\n",
              " ['03', '01', '06', '02', '01', '01', '08'],\n",
              " ['03', '01', '05', '01', '02', '02', '08'],\n",
              " ['03', '01', '03', '01', '02', '02', '08'],\n",
              " ['03', '01', '06', '01', '02', '01', '08'],\n",
              " ['03', '01', '06', '01', '01', '02', '08'],\n",
              " ['03', '01', '05', '01', '02', '01', '08'],\n",
              " ['03', '01', '04', '01', '01', '02', '08'],\n",
              " ['03', '01', '07', '01', '02', '01', '08'],\n",
              " ['03', '01', '08', '02', '01', '01', '08'],\n",
              " ['03', '01', '06', '02', '02', '01', '08'],\n",
              " ['03', '01', '07', '02', '01', '01', '08'],\n",
              " ['03', '01', '03', '01', '01', '01', '08'],\n",
              " ['03', '01', '02', '01', '02', '01', '08'],\n",
              " ['03', '01', '05', '02', '02', '02', '08'],\n",
              " ['03', '01', '03', '02', '01', '01', '08'],\n",
              " ['03', '01', '08', '01', '01', '01', '08'],\n",
              " ['03', '01', '02', '01', '01', '01', '08'],\n",
              " ['03', '01', '01', '01', '02', '02', '08'],\n",
              " ['03', '01', '04', '02', '01', '02', '08'],\n",
              " ['03', '01', '06', '01', '01', '01', '08'],\n",
              " ['03', '01', '06', '02', '02', '02', '08'],\n",
              " ['03', '01', '05', '02', '01', '01', '08'],\n",
              " ['03', '01', '08', '02', '02', '01', '08'],\n",
              " ['03', '01', '07', '02', '01', '02', '08'],\n",
              " ['03', '01', '08', '02', '01', '02', '08'],\n",
              " ['03', '01', '08', '01', '02', '01', '08'],\n",
              " ['03', '01', '04', '02', '02', '01', '08'],\n",
              " ['03', '01', '01', '01', '02', '01', '08'],\n",
              " ['03', '01', '04', '01', '02', '01', '08'],\n",
              " ['03', '01', '01', '01', '01', '01', '08'],\n",
              " ['03', '01', '04', '01', '01', '01', '08'],\n",
              " ['03', '01', '02', '02', '01', '01', '08'],\n",
              " ['03', '01', '05', '01', '01', '01', '08'],\n",
              " ['03', '01', '07', '01', '02', '02', '08'],\n",
              " ['03', '01', '05', '02', '02', '01', '08'],\n",
              " ['03', '01', '03', '02', '02', '01', '08'],\n",
              " ['03', '01', '08', '01', '02', '02', '08'],\n",
              " ['03', '01', '05', '01', '01', '02', '08'],\n",
              " ['03', '01', '07', '01', '01', '01', '08'],\n",
              " ['03', '01', '02', '01', '02', '02', '08'],\n",
              " ['03', '01', '02', '02', '02', '01', '08'],\n",
              " ['03', '01', '08', '01', '01', '02', '08'],\n",
              " ['03', '01', '01', '01', '01', '02', '08'],\n",
              " ['03', '01', '02', '02', '02', '02', '08'],\n",
              " ['03', '01', '03', '02', '01', '02', '08'],\n",
              " ['03', '01', '07', '02', '02', '01', '08'],\n",
              " ['03', '01', '03', '01', '01', '02', '08'],\n",
              " ['03', '01', '06', '02', '01', '02', '08'],\n",
              " ['03', '01', '05', '02', '01', '02', '08'],\n",
              " ['03', '01', '07', '02', '02', '02', '08'],\n",
              " ['03', '01', '08', '01', '01', '02', '09'],\n",
              " ['03', '01', '05', '02', '02', '02', '09'],\n",
              " ['03', '01', '03', '01', '02', '01', '09'],\n",
              " ['03', '01', '02', '01', '02', '02', '09'],\n",
              " ['03', '01', '07', '01', '02', '01', '09'],\n",
              " ['03', '01', '02', '02', '01', '02', '09'],\n",
              " ['03', '01', '06', '01', '02', '02', '09'],\n",
              " ['03', '01', '04', '02', '01', '02', '09'],\n",
              " ['03', '01', '06', '02', '02', '01', '09'],\n",
              " ['03', '01', '06', '01', '01', '02', '09'],\n",
              " ['03', '01', '04', '02', '02', '01', '09'],\n",
              " ['03', '01', '05', '02', '01', '02', '09'],\n",
              " ['03', '01', '03', '02', '01', '01', '09'],\n",
              " ['03', '01', '08', '02', '02', '02', '09'],\n",
              " ['03', '01', '08', '02', '01', '02', '09'],\n",
              " ['03', '01', '02', '01', '01', '02', '09'],\n",
              " ['03', '01', '04', '01', '02', '01', '09'],\n",
              " ['03', '01', '02', '01', '01', '01', '09'],\n",
              " ['03', '01', '03', '01', '01', '02', '09'],\n",
              " ['03', '01', '03', '01', '02', '02', '09'],\n",
              " ['03', '01', '07', '01', '01', '01', '09'],\n",
              " ['03', '01', '05', '01', '01', '02', '09'],\n",
              " ['03', '01', '01', '01', '01', '01', '09'],\n",
              " ['03', '01', '05', '01', '02', '02', '09'],\n",
              " ['03', '01', '01', '01', '02', '02', '09'],\n",
              " ['03', '01', '08', '02', '01', '01', '09'],\n",
              " ['03', '01', '06', '02', '01', '02', '09'],\n",
              " ['03', '01', '02', '02', '01', '01', '09'],\n",
              " ['03', '01', '05', '02', '01', '01', '09'],\n",
              " ['03', '01', '06', '02', '01', '01', '09'],\n",
              " ['03', '01', '04', '01', '02', '02', '09'],\n",
              " ['03', '01', '06', '02', '02', '02', '09'],\n",
              " ['03', '01', '04', '02', '01', '01', '09'],\n",
              " ['03', '01', '08', '02', '02', '01', '09'],\n",
              " ['03', '01', '02', '02', '02', '01', '09'],\n",
              " ['03', '01', '07', '01', '02', '02', '09'],\n",
              " ['03', '01', '03', '02', '02', '02', '09'],\n",
              " ['03', '01', '05', '01', '01', '01', '09'],\n",
              " ['03', '01', '06', '01', '01', '01', '09'],\n",
              " ['03', '01', '07', '02', '01', '02', '09'],\n",
              " ['03', '01', '08', '01', '02', '01', '09'],\n",
              " ['03', '01', '07', '02', '02', '02', '09'],\n",
              " ['03', '01', '04', '01', '01', '01', '09'],\n",
              " ['03', '01', '05', '02', '02', '01', '09'],\n",
              " ['03', '01', '01', '01', '02', '01', '09'],\n",
              " ['03', '01', '04', '01', '01', '02', '09'],\n",
              " ['03', '01', '08', '01', '01', '01', '09'],\n",
              " ['03', '01', '08', '01', '02', '02', '09'],\n",
              " ['03', '01', '04', '02', '02', '02', '09'],\n",
              " ['03', '01', '07', '02', '02', '01', '09'],\n",
              " ['03', '01', '05', '01', '02', '01', '09'],\n",
              " ['03', '01', '03', '01', '01', '01', '09'],\n",
              " ['03', '01', '02', '02', '02', '02', '09'],\n",
              " ['03', '01', '02', '01', '02', '01', '09'],\n",
              " ['03', '01', '07', '01', '01', '02', '09'],\n",
              " ['03', '01', '01', '01', '01', '02', '09'],\n",
              " ['03', '01', '03', '02', '02', '01', '09'],\n",
              " ['03', '01', '03', '02', '01', '02', '09'],\n",
              " ['03', '01', '07', '02', '01', '01', '09'],\n",
              " ['03', '01', '06', '01', '02', '01', '09'],\n",
              " ['03', '01', '04', '02', '01', '01', '10'],\n",
              " ['03', '01', '05', '01', '01', '02', '10'],\n",
              " ['03', '01', '07', '02', '01', '02', '10'],\n",
              " ['03', '01', '07', '01', '02', '01', '10'],\n",
              " ['03', '01', '07', '02', '02', '02', '10'],\n",
              " ['03', '01', '02', '02', '02', '02', '10'],\n",
              " ['03', '01', '06', '02', '01', '02', '10'],\n",
              " ['03', '01', '03', '02', '02', '01', '10'],\n",
              " ['03', '01', '06', '01', '01', '01', '10'],\n",
              " ['03', '01', '07', '02', '01', '01', '10'],\n",
              " ['03', '01', '04', '01', '02', '02', '10'],\n",
              " ['03', '01', '04', '01', '01', '01', '10'],\n",
              " ['03', '01', '08', '01', '01', '01', '10'],\n",
              " ['03', '01', '05', '02', '02', '02', '10'],\n",
              " ['03', '01', '05', '02', '01', '01', '10'],\n",
              " ['03', '01', '05', '01', '02', '01', '10'],\n",
              " ['03', '01', '07', '01', '02', '02', '10'],\n",
              " ['03', '01', '05', '01', '02', '02', '10'],\n",
              " ['03', '01', '02', '01', '01', '02', '10'],\n",
              " ['03', '01', '04', '02', '01', '02', '10'],\n",
              " ['03', '01', '08', '02', '02', '01', '10'],\n",
              " ['03', '01', '02', '01', '02', '01', '10'],\n",
              " ['03', '01', '04', '02', '02', '01', '10'],\n",
              " ['03', '01', '02', '02', '02', '01', '10'],\n",
              " ['03', '01', '06', '01', '01', '02', '10'],\n",
              " ['03', '01', '01', '01', '02', '02', '10'],\n",
              " ['03', '01', '01', '01', '01', '02', '10'],\n",
              " ['03', '01', '02', '01', '02', '02', '10'],\n",
              " ['03', '01', '06', '02', '01', '01', '10'],\n",
              " ['03', '01', '06', '01', '02', '01', '10'],\n",
              " ['03', '01', '03', '01', '01', '01', '10'],\n",
              " ['03', '01', '03', '02', '01', '02', '10'],\n",
              " ['03', '01', '08', '02', '01', '02', '10'],\n",
              " ['03', '01', '07', '01', '01', '01', '10'],\n",
              " ['03', '01', '08', '01', '02', '01', '10'],\n",
              " ['03', '01', '03', '01', '02', '02', '10'],\n",
              " ['03', '01', '06', '02', '02', '02', '10'],\n",
              " ['03', '01', '06', '01', '02', '02', '10'],\n",
              " ['03', '01', '03', '02', '01', '01', '10'],\n",
              " ['03', '01', '02', '02', '01', '01', '10'],\n",
              " ['03', '01', '05', '02', '01', '02', '10'],\n",
              " ['03', '01', '07', '02', '02', '01', '10'],\n",
              " ['03', '01', '08', '01', '02', '02', '10'],\n",
              " ['03', '01', '06', '02', '02', '01', '10'],\n",
              " ['03', '01', '03', '01', '02', '01', '10'],\n",
              " ['03', '01', '01', '01', '01', '01', '10'],\n",
              " ['03', '01', '05', '02', '02', '01', '10'],\n",
              " ['03', '01', '02', '02', '01', '02', '10'],\n",
              " ['03', '01', '08', '01', '01', '02', '10'],\n",
              " ['03', '01', '04', '01', '02', '01', '10'],\n",
              " ['03', '01', '02', '01', '01', '01', '10'],\n",
              " ['03', '01', '01', '01', '02', '01', '10'],\n",
              " ['03', '01', '08', '02', '02', '02', '10'],\n",
              " ['03', '01', '04', '02', '02', '02', '10'],\n",
              " ['03', '01', '03', '01', '01', '02', '10'],\n",
              " ['03', '01', '07', '01', '01', '02', '10'],\n",
              " ['03', '01', '03', '02', '02', '02', '10'],\n",
              " ['03', '01', '08', '02', '01', '01', '10'],\n",
              " ['03', '01', '04', '01', '01', '02', '10'],\n",
              " ['03', '01', '05', '01', '01', '01', '10'],\n",
              " ['03', '01', '02', '02', '01', '01', '11'],\n",
              " ['03', '01', '01', '01', '01', '01', '11'],\n",
              " ['03', '01', '05', '02', '02', '01', '11'],\n",
              " ['03', '01', '03', '01', '01', '02', '11'],\n",
              " ['03', '01', '06', '01', '01', '02', '11'],\n",
              " ['03', '01', '02', '01', '01', '01', '11'],\n",
              " ['03', '01', '07', '01', '01', '02', '11'],\n",
              " ['03', '01', '01', '01', '01', '02', '11'],\n",
              " ['03', '01', '07', '02', '01', '01', '11'],\n",
              " ['03', '01', '03', '02', '02', '02', '11'],\n",
              " ['03', '01', '03', '01', '01', '01', '11'],\n",
              " ['03', '01', '08', '01', '02', '01', '11'],\n",
              " ['03', '01', '05', '01', '02', '01', '11'],\n",
              " ['03', '01', '03', '02', '02', '01', '11'],\n",
              " ['03', '01', '02', '01', '01', '02', '11'],\n",
              " ['03', '01', '06', '01', '02', '02', '11'],\n",
              " ['03', '01', '03', '01', '02', '01', '11'],\n",
              " ['03', '01', '06', '02', '01', '02', '11'],\n",
              " ['03', '01', '06', '01', '01', '01', '11'],\n",
              " ['03', '01', '02', '02', '01', '02', '11'],\n",
              " ['03', '01', '06', '02', '01', '01', '11'],\n",
              " ['03', '01', '07', '01', '01', '01', '11'],\n",
              " ['03', '01', '04', '02', '01', '02', '11'],\n",
              " ['03', '01', '08', '01', '01', '01', '11'],\n",
              " ['03', '01', '02', '01', '02', '02', '11'],\n",
              " ['03', '01', '04', '02', '02', '01', '11'],\n",
              " ['03', '01', '08', '02', '02', '01', '11'],\n",
              " ['03', '01', '08', '02', '01', '01', '11'],\n",
              " ['03', '01', '05', '01', '01', '01', '11'],\n",
              " ['03', '01', '06', '01', '02', '01', '11'],\n",
              " ['03', '01', '06', '02', '02', '02', '11'],\n",
              " ['03', '01', '08', '02', '02', '02', '11'],\n",
              " ['03', '01', '07', '02', '02', '02', '11'],\n",
              " ['03', '01', '03', '02', '01', '02', '11'],\n",
              " ['03', '01', '04', '02', '02', '02', '11'],\n",
              " ['03', '01', '05', '01', '01', '02', '11'],\n",
              " ['03', '01', '04', '02', '01', '01', '11'],\n",
              " ['03', '01', '03', '02', '01', '01', '11'],\n",
              " ['03', '01', '04', '01', '02', '01', '11'],\n",
              " ['03', '01', '02', '02', '02', '02', '11'],\n",
              " ['03', '01', '08', '01', '02', '02', '11'],\n",
              " ['03', '01', '06', '02', '02', '01', '11'],\n",
              " ['03', '01', '02', '02', '02', '01', '11'],\n",
              " ['03', '01', '08', '02', '01', '02', '11'],\n",
              " ['03', '01', '05', '02', '01', '01', '11'],\n",
              " ['03', '01', '04', '01', '01', '01', '11'],\n",
              " ['03', '01', '07', '02', '02', '01', '11'],\n",
              " ['03', '01', '05', '01', '02', '02', '11'],\n",
              " ['03', '01', '05', '02', '01', '02', '11'],\n",
              " ['03', '01', '07', '01', '02', '01', '11'],\n",
              " ['03', '01', '04', '01', '01', '02', '11'],\n",
              " ['03', '01', '01', '01', '02', '02', '11'],\n",
              " ['03', '01', '07', '02', '01', '02', '11'],\n",
              " ['03', '01', '08', '01', '01', '02', '11'],\n",
              " ['03', '01', '01', '01', '02', '01', '11'],\n",
              " ['03', '01', '03', '01', '02', '02', '11'],\n",
              " ['03', '01', '04', '01', '02', '02', '11'],\n",
              " ['03', '01', '07', '01', '02', '02', '11'],\n",
              " ['03', '01', '05', '02', '02', '02', '11'],\n",
              " ['03', '01', '02', '01', '02', '01', '11'],\n",
              " ['03', '01', '08', '01', '02', '01', '12'],\n",
              " ['03', '01', '02', '02', '01', '02', '12'],\n",
              " ['03', '01', '03', '01', '02', '02', '12'],\n",
              " ['03', '01', '05', '01', '02', '01', '12'],\n",
              " ['03', '01', '06', '01', '02', '01', '12'],\n",
              " ['03', '01', '05', '02', '01', '02', '12'],\n",
              " ['03', '01', '02', '01', '02', '02', '12'],\n",
              " ['03', '01', '03', '02', '02', '01', '12'],\n",
              " ['03', '01', '04', '01', '02', '01', '12'],\n",
              " ['03', '01', '02', '01', '01', '02', '12'],\n",
              " ['03', '01', '02', '02', '02', '01', '12'],\n",
              " ['03', '01', '04', '02', '02', '01', '12'],\n",
              " ['03', '01', '07', '02', '02', '02', '12'],\n",
              " ['03', '01', '01', '01', '02', '02', '12'],\n",
              " ['03', '01', '08', '02', '01', '02', '12'],\n",
              " ['03', '01', '01', '01', '01', '02', '12'],\n",
              " ['03', '01', '05', '01', '01', '01', '12'],\n",
              " ['03', '01', '02', '02', '02', '02', '12'],\n",
              " ['03', '01', '02', '02', '01', '01', '12'],\n",
              " ['03', '01', '01', '01', '01', '01', '12'],\n",
              " ['03', '01', '07', '01', '02', '01', '12'],\n",
              " ['03', '01', '08', '02', '02', '01', '12'],\n",
              " ['03', '01', '08', '01', '02', '02', '12'],\n",
              " ['03', '01', '04', '02', '02', '02', '12'],\n",
              " ['03', '01', '03', '01', '01', '02', '12'],\n",
              " ['03', '01', '04', '01', '01', '01', '12'],\n",
              " ['03', '01', '07', '02', '01', '01', '12'],\n",
              " ['03', '01', '01', '01', '02', '01', '12'],\n",
              " ['03', '01', '04', '01', '01', '02', '12'],\n",
              " ['03', '01', '05', '01', '01', '02', '12'],\n",
              " ['03', '01', '03', '01', '02', '01', '12'],\n",
              " ['03', '01', '05', '02', '01', '01', '12'],\n",
              " ['03', '01', '04', '01', '02', '02', '12'],\n",
              " ['03', '01', '08', '01', '01', '01', '12'],\n",
              " ['03', '01', '06', '02', '02', '01', '12'],\n",
              " ['03', '01', '08', '02', '01', '01', '12'],\n",
              " ['03', '01', '03', '02', '01', '01', '12'],\n",
              " ['03', '01', '05', '02', '02', '02', '12'],\n",
              " ['03', '01', '03', '01', '01', '01', '12'],\n",
              " ['03', '01', '06', '01', '02', '02', '12'],\n",
              " ['03', '01', '02', '01', '02', '01', '12'],\n",
              " ['03', '01', '07', '01', '01', '02', '12'],\n",
              " ['03', '01', '06', '02', '01', '02', '12'],\n",
              " ['03', '01', '08', '02', '02', '02', '12'],\n",
              " ['03', '01', '06', '01', '01', '01', '12'],\n",
              " ['03', '01', '02', '01', '01', '01', '12'],\n",
              " ['03', '01', '05', '02', '02', '01', '12'],\n",
              " ['03', '01', '03', '02', '01', '02', '12'],\n",
              " ['03', '01', '04', '02', '01', '01', '12'],\n",
              " ['03', '01', '05', '01', '02', '02', '12'],\n",
              " ['03', '01', '04', '02', '01', '02', '12'],\n",
              " ['03', '01', '07', '01', '01', '01', '12'],\n",
              " ['03', '01', '06', '02', '01', '01', '12'],\n",
              " ['03', '01', '07', '02', '02', '01', '12'],\n",
              " ['03', '01', '06', '02', '02', '02', '12'],\n",
              " ['03', '01', '08', '01', '01', '02', '12'],\n",
              " ['03', '01', '07', '01', '02', '02', '12'],\n",
              " ['03', '01', '06', '01', '01', '02', '12'],\n",
              " ['03', '01', '03', '02', '02', '02', '12'],\n",
              " ['03', '01', '07', '02', '01', '02', '12'],\n",
              " ['03', '01', '01', '01', '01', '02', '13'],\n",
              " ['03', '01', '04', '01', '02', '02', '13'],\n",
              " ['03', '01', '05', '02', '01', '02', '13'],\n",
              " ['03', '01', '05', '01', '01', '02', '13'],\n",
              " ['03', '01', '03', '02', '01', '01', '13'],\n",
              " ['03', '01', '04', '02', '01', '01', '13'],\n",
              " ['03', '01', '04', '02', '01', '02', '13'],\n",
              " ['03', '01', '02', '02', '02', '01', '13'],\n",
              " ['03', '01', '03', '02', '02', '01', '13'],\n",
              " ['03', '01', '05', '02', '02', '02', '13'],\n",
              " ['03', '01', '05', '01', '01', '01', '13'],\n",
              " ['03', '01', '01', '01', '02', '02', '13'],\n",
              " ['03', '01', '08', '02', '01', '01', '13'],\n",
              " ['03', '01', '01', '01', '02', '01', '13'],\n",
              " ['03', '01', '03', '01', '01', '02', '13'],\n",
              " ['03', '01', '05', '02', '01', '01', '13'],\n",
              " ['03', '01', '03', '01', '02', '01', '13'],\n",
              " ['03', '01', '06', '02', '02', '02', '13'],\n",
              " ['03', '01', '07', '01', '02', '01', '13'],\n",
              " ['03', '01', '08', '02', '02', '01', '13'],\n",
              " ['03', '01', '07', '01', '02', '02', '13'],\n",
              " ['03', '01', '02', '02', '01', '01', '13'],\n",
              " ['03', '01', '06', '02', '01', '01', '13'],\n",
              " ['03', '01', '02', '02', '01', '02', '13'],\n",
              " ['03', '01', '02', '01', '02', '01', '13'],\n",
              " ['03', '01', '07', '02', '02', '01', '13'],\n",
              " ['03', '01', '06', '01', '01', '02', '13'],\n",
              " ['03', '01', '04', '02', '02', '02', '13'],\n",
              " ['03', '01', '08', '01', '02', '02', '13'],\n",
              " ['03', '01', '06', '01', '02', '01', '13'],\n",
              " ['03', '01', '02', '01', '01', '02', '13'],\n",
              " ['03', '01', '06', '02', '01', '02', '13'],\n",
              " ['03', '01', '04', '02', '02', '01', '13'],\n",
              " ['03', '01', '06', '01', '02', '02', '13'],\n",
              " ['03', '01', '01', '01', '01', '01', '13'],\n",
              " ['03', '01', '07', '01', '01', '01', '13'],\n",
              " ['03', '01', '07', '02', '02', '02', '13'],\n",
              " ['03', '01', '08', '01', '02', '01', '13'],\n",
              " ['03', '01', '05', '02', '02', '01', '13'],\n",
              " ['03', '01', '04', '01', '01', '02', '13'],\n",
              " ['03', '01', '04', '01', '01', '01', '13'],\n",
              " ['03', '01', '06', '02', '02', '01', '13'],\n",
              " ['03', '01', '05', '01', '02', '02', '13'],\n",
              " ['03', '01', '07', '01', '01', '02', '13'],\n",
              " ['03', '01', '06', '01', '01', '01', '13'],\n",
              " ['03', '01', '02', '01', '02', '02', '13'],\n",
              " ['03', '01', '03', '01', '01', '01', '13'],\n",
              " ['03', '01', '04', '01', '02', '01', '13'],\n",
              " ['03', '01', '05', '01', '02', '01', '13'],\n",
              " ['03', '01', '08', '02', '01', '02', '13'],\n",
              " ['03', '01', '03', '02', '01', '02', '13'],\n",
              " ['03', '01', '08', '02', '02', '02', '13'],\n",
              " ['03', '01', '03', '01', '02', '02', '13'],\n",
              " ['03', '01', '07', '02', '01', '01', '13'],\n",
              " ['03', '01', '03', '02', '02', '02', '13'],\n",
              " ['03', '01', '08', '01', '01', '02', '13'],\n",
              " ['03', '01', '07', '02', '01', '02', '13'],\n",
              " ['03', '01', '02', '01', '01', '01', '13'],\n",
              " ['03', '01', '08', '01', '01', '01', '13'],\n",
              " ['03', '01', '02', '02', '02', '02', '13'],\n",
              " ['03', '01', '04', '01', '01', '01', '14'],\n",
              " ['03', '01', '05', '02', '02', '01', '14'],\n",
              " ['03', '01', '05', '01', '02', '02', '14'],\n",
              " ['03', '01', '01', '01', '01', '01', '14'],\n",
              " ['03', '01', '08', '02', '02', '01', '14'],\n",
              " ['03', '01', '01', '01', '01', '02', '14'],\n",
              " ['03', '01', '07', '02', '01', '01', '14'],\n",
              " ['03', '01', '02', '02', '01', '01', '14'],\n",
              " ['03', '01', '05', '01', '02', '01', '14'],\n",
              " ['03', '01', '01', '01', '02', '02', '14'],\n",
              " ['03', '01', '06', '01', '02', '02', '14'],\n",
              " ['03', '01', '08', '02', '02', '02', '14'],\n",
              " ['03', '01', '04', '02', '02', '01', '14'],\n",
              " ['03', '01', '07', '01', '02', '02', '14'],\n",
              " ['03', '01', '04', '01', '01', '02', '14'],\n",
              " ['03', '01', '03', '01', '02', '01', '14'],\n",
              " ['03', '01', '07', '02', '01', '02', '14'],\n",
              " ['03', '01', '03', '02', '01', '02', '14'],\n",
              " ['03', '01', '07', '01', '01', '01', '14'],\n",
              " ['03', '01', '08', '01', '02', '02', '14'],\n",
              " ['03', '01', '04', '02', '02', '02', '14'],\n",
              " ['03', '01', '05', '02', '01', '02', '14'],\n",
              " ['03', '01', '06', '01', '01', '02', '14'],\n",
              " ['03', '01', '05', '02', '02', '02', '14'],\n",
              " ['03', '01', '07', '01', '01', '02', '14'],\n",
              " ['03', '01', '08', '01', '01', '01', '14'],\n",
              " ['03', '01', '04', '01', '02', '02', '14'],\n",
              " ['03', '01', '06', '02', '02', '02', '14'],\n",
              " ['03', '01', '05', '02', '01', '01', '14'],\n",
              " ['03', '01', '07', '01', '02', '01', '14'],\n",
              " ['03', '01', '06', '01', '02', '01', '14'],\n",
              " ['03', '01', '08', '01', '02', '01', '14'],\n",
              " ['03', '01', '07', '02', '02', '02', '14'],\n",
              " ['03', '01', '02', '01', '02', '01', '14'],\n",
              " ['03', '01', '02', '01', '02', '02', '14'],\n",
              " ['03', '01', '08', '01', '01', '02', '14'],\n",
              " ['03', '01', '03', '01', '01', '01', '14'],\n",
              " ['03', '01', '08', '02', '01', '01', '14'],\n",
              " ['03', '01', '06', '02', '02', '01', '14'],\n",
              " ['03', '01', '04', '02', '01', '02', '14'],\n",
              " ['03', '01', '03', '02', '02', '02', '14'],\n",
              " ['03', '01', '04', '02', '01', '01', '14'],\n",
              " ['03', '01', '06', '02', '01', '01', '14'],\n",
              " ['03', '01', '03', '01', '02', '02', '14'],\n",
              " ['03', '01', '01', '01', '02', '01', '14'],\n",
              " ['03', '01', '03', '01', '01', '02', '14'],\n",
              " ['03', '01', '02', '02', '01', '02', '14'],\n",
              " ['03', '01', '05', '01', '01', '01', '14'],\n",
              " ['03', '01', '03', '02', '01', '01', '14'],\n",
              " ['03', '01', '06', '02', '01', '02', '14'],\n",
              " ['03', '01', '04', '01', '02', '01', '14'],\n",
              " ['03', '01', '07', '02', '02', '01', '14'],\n",
              " ['03', '01', '02', '02', '02', '01', '14'],\n",
              " ['03', '01', '05', '01', '01', '02', '14'],\n",
              " ['03', '01', '02', '01', '01', '02', '14'],\n",
              " ['03', '01', '03', '02', '02', '01', '14'],\n",
              " ['03', '01', '06', '01', '01', '01', '14'],\n",
              " ['03', '01', '08', '02', '01', '02', '14'],\n",
              " ['03', '01', '02', '02', '02', '02', '14'],\n",
              " ['03', '01', '02', '01', '01', '01', '14'],\n",
              " ['03', '01', '05', '02', '01', '01', '15'],\n",
              " ['03', '01', '02', '02', '01', '01', '15'],\n",
              " ['03', '01', '08', '02', '02', '01', '15'],\n",
              " ['03', '01', '06', '02', '02', '02', '15'],\n",
              " ['03', '01', '05', '01', '02', '02', '15'],\n",
              " ['03', '01', '02', '02', '01', '02', '15'],\n",
              " ['03', '01', '02', '02', '02', '02', '15'],\n",
              " ['03', '01', '01', '01', '02', '02', '15'],\n",
              " ['03', '01', '01', '01', '01', '01', '15'],\n",
              " ['03', '01', '08', '01', '02', '01', '15'],\n",
              " ['03', '01', '03', '01', '01', '01', '15'],\n",
              " ['03', '01', '01', '01', '02', '01', '15'],\n",
              " ['03', '01', '02', '01', '01', '01', '15'],\n",
              " ['03', '01', '06', '02', '01', '02', '15'],\n",
              " ['03', '01', '07', '01', '01', '01', '15'],\n",
              " ['03', '01', '04', '01', '01', '02', '15'],\n",
              " ['03', '01', '02', '01', '02', '02', '15'],\n",
              " ['03', '01', '03', '01', '02', '02', '15'],\n",
              " ['03', '01', '03', '01', '02', '01', '15'],\n",
              " ['03', '01', '03', '02', '01', '01', '15'],\n",
              " ['03', '01', '04', '02', '02', '01', '15'],\n",
              " ['03', '01', '07', '02', '02', '01', '15'],\n",
              " ['03', '01', '07', '02', '01', '02', '15'],\n",
              " ['03', '01', '07', '01', '01', '02', '15'],\n",
              " ['03', '01', '07', '02', '01', '01', '15'],\n",
              " ['03', '01', '08', '02', '01', '01', '15'],\n",
              " ['03', '01', '08', '02', '02', '02', '15'],\n",
              " ['03', '01', '01', '01', '01', '02', '15'],\n",
              " ['03', '01', '03', '01', '01', '02', '15'],\n",
              " ['03', '01', '04', '02', '02', '02', '15'],\n",
              " ['03', '01', '04', '01', '02', '01', '15'],\n",
              " ['03', '01', '04', '02', '01', '01', '15'],\n",
              " ['03', '01', '06', '02', '02', '01', '15'],\n",
              " ['03', '01', '02', '01', '02', '01', '15'],\n",
              " ['03', '01', '03', '02', '01', '02', '15'],\n",
              " ['03', '01', '03', '02', '02', '01', '15'],\n",
              " ['03', '01', '06', '01', '02', '02', '15'],\n",
              " ['03', '01', '08', '02', '01', '02', '15'],\n",
              " ['03', '01', '04', '02', '01', '02', '15'],\n",
              " ['03', '01', '05', '01', '02', '01', '15'],\n",
              " ['03', '01', '08', '01', '01', '01', '15'],\n",
              " ['03', '01', '06', '01', '01', '01', '15'],\n",
              " ['03', '01', '04', '01', '01', '01', '15'],\n",
              " ['03', '01', '04', '01', '02', '02', '15'],\n",
              " ['03', '01', '05', '01', '01', '02', '15'],\n",
              " ['03', '01', '06', '01', '01', '02', '15'],\n",
              " ['03', '01', '05', '02', '02', '01', '15'],\n",
              " ['03', '01', '07', '02', '02', '02', '15'],\n",
              " ['03', '01', '06', '01', '02', '01', '15'],\n",
              " ['03', '01', '07', '01', '02', '02', '15'],\n",
              " ['03', '01', '08', '01', '02', '02', '15'],\n",
              " ['03', '01', '05', '02', '01', '02', '15'],\n",
              " ['03', '01', '02', '02', '02', '01', '15'],\n",
              " ['03', '01', '03', '02', '02', '02', '15'],\n",
              " ['03', '01', '05', '02', '02', '02', '15'],\n",
              " ['03', '01', '06', '02', '01', '01', '15'],\n",
              " ['03', '01', '07', '01', '02', '01', '15'],\n",
              " ['03', '01', '02', '01', '01', '02', '15'],\n",
              " ['03', '01', '05', '01', '01', '01', '15'],\n",
              " ['03', '01', '08', '01', '01', '02', '15'],\n",
              " ['03', '01', '05', '02', '01', '01', '16'],\n",
              " ['03', '01', '08', '02', '01', '01', '16'],\n",
              " ['03', '01', '03', '02', '01', '01', '16'],\n",
              " ['03', '01', '07', '02', '02', '02', '16'],\n",
              " ['03', '01', '06', '01', '02', '01', '16'],\n",
              " ['03', '01', '01', '01', '02', '01', '16'],\n",
              " ['03', '01', '04', '01', '01', '02', '16'],\n",
              " ['03', '01', '04', '01', '01', '01', '16'],\n",
              " ['03', '01', '02', '02', '01', '01', '16'],\n",
              " ['03', '01', '06', '02', '02', '02', '16'],\n",
              " ['03', '01', '07', '01', '01', '02', '16'],\n",
              " ['03', '01', '04', '02', '02', '01', '16'],\n",
              " ['03', '01', '07', '01', '02', '01', '16'],\n",
              " ['03', '01', '06', '01', '01', '02', '16'],\n",
              " ['03', '01', '03', '01', '02', '02', '16'],\n",
              " ['03', '01', '04', '02', '02', '02', '16'],\n",
              " ['03', '01', '02', '02', '01', '02', '16'],\n",
              " ['03', '01', '05', '01', '01', '01', '16'],\n",
              " ['03', '01', '07', '02', '01', '02', '16'],\n",
              " ['03', '01', '03', '02', '01', '02', '16'],\n",
              " ['03', '01', '07', '01', '02', '02', '16'],\n",
              " ['03', '01', '01', '01', '02', '02', '16'],\n",
              " ['03', '01', '01', '01', '01', '01', '16'],\n",
              " ['03', '01', '08', '02', '02', '01', '16'],\n",
              " ['03', '01', '06', '02', '01', '02', '16'],\n",
              " ['03', '01', '06', '01', '02', '02', '16'],\n",
              " ['03', '01', '03', '02', '02', '01', '16'],\n",
              " ['03', '01', '01', '01', '01', '02', '16'],\n",
              " ['03', '01', '07', '01', '01', '01', '16'],\n",
              " ['03', '01', '02', '02', '02', '02', '16'],\n",
              " ['03', '01', '05', '02', '02', '02', '16'],\n",
              " ['03', '01', '02', '02', '02', '01', '16'],\n",
              " ['03', '01', '03', '01', '01', '01', '16'],\n",
              " ['03', '01', '07', '02', '02', '01', '16'],\n",
              " ['03', '01', '03', '02', '02', '02', '16'],\n",
              " ['03', '01', '02', '01', '02', '02', '16'],\n",
              " ['03', '01', '04', '01', '02', '01', '16'],\n",
              " ['03', '01', '06', '02', '02', '01', '16'],\n",
              " ['03', '01', '04', '02', '01', '02', '16'],\n",
              " ['03', '01', '05', '02', '02', '01', '16'],\n",
              " ['03', '01', '06', '01', '01', '01', '16'],\n",
              " ['03', '01', '04', '02', '01', '01', '16'],\n",
              " ['03', '01', '07', '02', '01', '01', '16'],\n",
              " ['03', '01', '02', '01', '02', '01', '16'],\n",
              " ['03', '01', '08', '01', '02', '02', '16'],\n",
              " ['03', '01', '08', '02', '02', '02', '16'],\n",
              " ['03', '01', '08', '01', '02', '01', '16'],\n",
              " ['03', '01', '06', '02', '01', '01', '16'],\n",
              " ['03', '01', '02', '01', '01', '01', '16'],\n",
              " ['03', '01', '08', '02', '01', '02', '16'],\n",
              " ['03', '01', '05', '01', '02', '01', '16'],\n",
              " ['03', '01', '03', '01', '01', '02', '16'],\n",
              " ['03', '01', '05', '01', '02', '02', '16'],\n",
              " ['03', '01', '03', '01', '02', '01', '16'],\n",
              " ['03', '01', '05', '02', '01', '02', '16'],\n",
              " ['03', '01', '02', '01', '01', '02', '16'],\n",
              " ['03', '01', '08', '01', '01', '01', '16'],\n",
              " ['03', '01', '08', '01', '01', '02', '16'],\n",
              " ['03', '01', '04', '01', '02', '02', '16'],\n",
              " ['03', '01', '05', '01', '01', '02', '16'],\n",
              " ['03', '01', '02', '01', '02', '02', '17'],\n",
              " ['03', '01', '07', '02', '02', '01', '17'],\n",
              " ['03', '01', '08', '01', '01', '02', '17'],\n",
              " ['03', '01', '04', '02', '02', '01', '17'],\n",
              " ['03', '01', '03', '02', '02', '02', '17'],\n",
              " ['03', '01', '06', '01', '01', '02', '17'],\n",
              " ['03', '01', '08', '02', '02', '02', '17'],\n",
              " ['03', '01', '03', '01', '01', '01', '17'],\n",
              " ['03', '01', '07', '02', '02', '02', '17'],\n",
              " ['03', '01', '07', '01', '02', '01', '17'],\n",
              " ['03', '01', '05', '02', '01', '02', '17'],\n",
              " ['03', '01', '04', '01', '01', '01', '17'],\n",
              " ['03', '01', '02', '02', '02', '01', '17'],\n",
              " ['03', '01', '05', '01', '02', '02', '17'],\n",
              " ['03', '01', '01', '01', '02', '01', '17'],\n",
              " ['03', '01', '03', '01', '01', '02', '17'],\n",
              " ['03', '01', '07', '02', '01', '01', '17'],\n",
              " ['03', '01', '04', '02', '01', '02', '17'],\n",
              " ['03', '01', '03', '02', '01', '02', '17'],\n",
              " ['03', '01', '05', '01', '02', '01', '17'],\n",
              " ['03', '01', '06', '01', '02', '02', '17'],\n",
              " ['03', '01', '01', '01', '01', '01', '17'],\n",
              " ['03', '01', '05', '02', '01', '01', '17'],\n",
              " ['03', '01', '08', '02', '02', '01', '17'],\n",
              " ['03', '01', '03', '01', '02', '01', '17'],\n",
              " ['03', '01', '07', '01', '02', '02', '17'],\n",
              " ['03', '01', '02', '01', '02', '01', '17'],\n",
              " ['03', '01', '02', '01', '01', '02', '17'],\n",
              " ['03', '01', '06', '02', '02', '01', '17'],\n",
              " ['03', '01', '06', '01', '02', '01', '17'],\n",
              " ['03', '01', '08', '01', '02', '02', '17'],\n",
              " ['03', '01', '02', '02', '02', '02', '17'],\n",
              " ['03', '01', '05', '02', '02', '02', '17'],\n",
              " ['03', '01', '07', '01', '01', '01', '17'],\n",
              " ['03', '01', '01', '01', '02', '02', '17'],\n",
              " ['03', '01', '02', '02', '01', '02', '17'],\n",
              " ['03', '01', '06', '01', '01', '01', '17'],\n",
              " ['03', '01', '05', '01', '01', '02', '17'],\n",
              " ['03', '01', '05', '02', '02', '01', '17'],\n",
              " ['03', '01', '05', '01', '01', '01', '17'],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HlxbLuQ3DNT",
        "outputId": "2e166cb4-efcf-4ec5-fe03-a3ac074d883c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1440"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = np.array(final).astype(int)"
      ],
      "metadata": {
        "id": "7YeYbZLNAqXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkLpFLjDAvz5",
        "outputId": "718d8d61-213c-494b-d963-c369c73b592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  1,  5, ...,  2,  1,  1],\n",
              "       [ 3,  1,  4, ...,  2,  1,  1],\n",
              "       [ 3,  1,  1, ...,  2,  2,  1],\n",
              "       ...,\n",
              "       [ 3,  1,  2, ...,  1,  2, 24],\n",
              "       [ 3,  1,  4, ...,  1,  2, 24],\n",
              "       [ 3,  1,  6, ...,  1,  1, 24]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(final,columns=[\"Modality\",\"Vocal channel\",\"Emotion\",\"Emotional intensity\",\"Statement\",\"Repetition\",\"Actor\"])"
      ],
      "metadata": {
        "id": "-UMZFTZZA41m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6iNsemETBAoa",
        "outputId": "7cc08468-cca9-4ee7-e755-c5e6d78a689b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Modality  Vocal channel  Emotion  Emotional intensity  Statement  \\\n",
              "0            3              1        5                    2          2   \n",
              "1            3              1        4                    2          2   \n",
              "2            3              1        1                    1          2   \n",
              "3            3              1        2                    2          2   \n",
              "4            3              1        4                    2          2   \n",
              "...        ...            ...      ...                  ...        ...   \n",
              "1435         3              1        8                    1          1   \n",
              "1436         3              1        5                    1          1   \n",
              "1437         3              1        2                    2          1   \n",
              "1438         3              1        4                    1          1   \n",
              "1439         3              1        6                    1          1   \n",
              "\n",
              "      Repetition  Actor  \n",
              "0              1      1  \n",
              "1              1      1  \n",
              "2              2      1  \n",
              "3              1      1  \n",
              "4              2      1  \n",
              "...          ...    ...  \n",
              "1435           2     24  \n",
              "1436           2     24  \n",
              "1437           2     24  \n",
              "1438           2     24  \n",
              "1439           1     24  \n",
              "\n",
              "[1440 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ece7abbf-4977-4412-9293-18c88735dcde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modality</th>\n",
              "      <th>Vocal channel</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotional intensity</th>\n",
              "      <th>Statement</th>\n",
              "      <th>Repetition</th>\n",
              "      <th>Actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows  7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ece7abbf-4977-4412-9293-18c88735dcde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ece7abbf-4977-4412-9293-18c88735dcde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ece7abbf-4977-4412-9293-18c88735dcde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio = pd.DataFrame(audio)"
      ],
      "metadata": {
        "id": "71xOy-mqTLMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "vZoBJaAKTW4_",
        "outputId": "c58b2fa7-1168-45ff-8f8c-804a37372e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0             1             2             3             4      \\\n",
              "0     2.329042e-04  1.441041e-03  3.753820e-04 -9.898245e-04 -7.679415e-04   \n",
              "1     1.018356e-08 -1.005014e-08  9.524251e-09 -8.538231e-09  7.048115e-09   \n",
              "2     2.278331e-05  3.115162e-05  4.793048e-06 -2.723841e-06  3.141930e-06   \n",
              "3     2.429188e-05  2.923215e-05  8.048310e-06  2.269307e-05  4.088906e-05   \n",
              "4     1.813627e-13 -1.941189e-13 -1.676032e-13 -3.610815e-14  8.697801e-14   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "1435  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1436  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1437  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1438  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1439 -1.072154e-10 -3.232369e-11  1.666647e-10 -3.448771e-10  5.652159e-10   \n",
              "\n",
              "             5             6             7             8             9      \\\n",
              "0    -4.599194e-04 -4.708872e-05  4.473904e-04  4.904005e-04  1.692135e-04   \n",
              "1    -4.995490e-09  2.328892e-09  9.916166e-10 -4.992537e-09  9.683954e-09   \n",
              "2    -4.905643e-06  3.262787e-05  6.475767e-05  3.161153e-05  3.185779e-05   \n",
              "3     5.115393e-05  4.717479e-05  2.486637e-05  4.857911e-05  4.697332e-05   \n",
              "4     1.488941e-13  3.876199e-14  1.113698e-13  3.546329e-13 -1.602816e-13   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "1435  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1436  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1437  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1438  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "1439 -8.308132e-10  1.141762e-09 -1.497725e-09  1.894819e-09 -2.328016e-09   \n",
              "\n",
              "      ...         63935     63936     63937         63938         63939  \\\n",
              "0     ...  4.532097e-02  0.050078  0.045070  3.029229e-02  3.539371e-02   \n",
              "1     ...  3.179810e-05  0.000044  0.000026 -1.672583e-06  9.198144e-06   \n",
              "2     ...  3.252479e-06  0.000013 -0.000002  3.341737e-07  5.308339e-07   \n",
              "3     ...  8.280606e-03  0.008378  0.008179  7.974692e-03  7.836316e-03   \n",
              "4     ... -5.929249e-06  0.000005 -0.000005  9.120397e-06  2.205470e-05   \n",
              "...   ...           ...       ...       ...           ...           ...   \n",
              "1435  ... -3.476922e-07 -0.000001  0.000004  3.806172e-05  3.835830e-07   \n",
              "1436  ... -2.550414e-06  0.000002 -0.000002  1.629342e-06 -1.235343e-06   \n",
              "1437  ...  4.530484e-04  0.000148 -0.000035 -2.018864e-04 -2.044283e-04   \n",
              "1438  ...  8.580230e-04  0.000592  0.000535  3.330597e-04  1.518707e-04   \n",
              "1439  ...  3.091938e-02  0.030882  0.017899  2.623495e-02  1.683696e-02   \n",
              "\n",
              "             63940         63941         63942         63943         63944  \n",
              "0     5.646525e-02  5.664071e-02  5.869206e-02  5.510687e-02  6.350432e-02  \n",
              "1     2.301180e-05 -3.754093e-06  2.038091e-06 -1.250351e-06  7.715186e-07  \n",
              "2     1.392216e-05 -4.699850e-07 -2.686204e-08  2.657069e-07 -4.201772e-07  \n",
              "3     7.985077e-03  8.453447e-03  8.543141e-03  7.968901e-03  7.787952e-03  \n",
              "4     5.102658e-06  2.106591e-05  7.540939e-08  6.271493e-06  1.096370e-05  \n",
              "...            ...           ...           ...           ...           ...  \n",
              "1435  6.750817e-06  1.863219e-05  1.346969e-05  5.569830e-06 -3.534751e-06  \n",
              "1436  8.224735e-07 -5.586875e-07  2.645753e-06  5.533508e-05  4.465951e-05  \n",
              "1437 -1.777444e-04 -4.314623e-05  1.452080e-04  3.828391e-04  7.722454e-04  \n",
              "1438  1.039850e-04  7.744366e-05  1.735599e-05 -1.867246e-06 -5.283646e-06  \n",
              "1439 -9.608166e-04 -1.694898e-02 -3.794178e-02 -4.874028e-02 -5.946823e-02  \n",
              "\n",
              "[1440 rows x 63945 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b61d2e02-b7fb-473f-a029-305cf0ea0f1a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>63935</th>\n",
              "      <th>63936</th>\n",
              "      <th>63937</th>\n",
              "      <th>63938</th>\n",
              "      <th>63939</th>\n",
              "      <th>63940</th>\n",
              "      <th>63941</th>\n",
              "      <th>63942</th>\n",
              "      <th>63943</th>\n",
              "      <th>63944</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.329042e-04</td>\n",
              "      <td>1.441041e-03</td>\n",
              "      <td>3.753820e-04</td>\n",
              "      <td>-9.898245e-04</td>\n",
              "      <td>-7.679415e-04</td>\n",
              "      <td>-4.599194e-04</td>\n",
              "      <td>-4.708872e-05</td>\n",
              "      <td>4.473904e-04</td>\n",
              "      <td>4.904005e-04</td>\n",
              "      <td>1.692135e-04</td>\n",
              "      <td>...</td>\n",
              "      <td>4.532097e-02</td>\n",
              "      <td>0.050078</td>\n",
              "      <td>0.045070</td>\n",
              "      <td>3.029229e-02</td>\n",
              "      <td>3.539371e-02</td>\n",
              "      <td>5.646525e-02</td>\n",
              "      <td>5.664071e-02</td>\n",
              "      <td>5.869206e-02</td>\n",
              "      <td>5.510687e-02</td>\n",
              "      <td>6.350432e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.018356e-08</td>\n",
              "      <td>-1.005014e-08</td>\n",
              "      <td>9.524251e-09</td>\n",
              "      <td>-8.538231e-09</td>\n",
              "      <td>7.048115e-09</td>\n",
              "      <td>-4.995490e-09</td>\n",
              "      <td>2.328892e-09</td>\n",
              "      <td>9.916166e-10</td>\n",
              "      <td>-4.992537e-09</td>\n",
              "      <td>9.683954e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>3.179810e-05</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-1.672583e-06</td>\n",
              "      <td>9.198144e-06</td>\n",
              "      <td>2.301180e-05</td>\n",
              "      <td>-3.754093e-06</td>\n",
              "      <td>2.038091e-06</td>\n",
              "      <td>-1.250351e-06</td>\n",
              "      <td>7.715186e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.278331e-05</td>\n",
              "      <td>3.115162e-05</td>\n",
              "      <td>4.793048e-06</td>\n",
              "      <td>-2.723841e-06</td>\n",
              "      <td>3.141930e-06</td>\n",
              "      <td>-4.905643e-06</td>\n",
              "      <td>3.262787e-05</td>\n",
              "      <td>6.475767e-05</td>\n",
              "      <td>3.161153e-05</td>\n",
              "      <td>3.185779e-05</td>\n",
              "      <td>...</td>\n",
              "      <td>3.252479e-06</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>3.341737e-07</td>\n",
              "      <td>5.308339e-07</td>\n",
              "      <td>1.392216e-05</td>\n",
              "      <td>-4.699850e-07</td>\n",
              "      <td>-2.686204e-08</td>\n",
              "      <td>2.657069e-07</td>\n",
              "      <td>-4.201772e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.429188e-05</td>\n",
              "      <td>2.923215e-05</td>\n",
              "      <td>8.048310e-06</td>\n",
              "      <td>2.269307e-05</td>\n",
              "      <td>4.088906e-05</td>\n",
              "      <td>5.115393e-05</td>\n",
              "      <td>4.717479e-05</td>\n",
              "      <td>2.486637e-05</td>\n",
              "      <td>4.857911e-05</td>\n",
              "      <td>4.697332e-05</td>\n",
              "      <td>...</td>\n",
              "      <td>8.280606e-03</td>\n",
              "      <td>0.008378</td>\n",
              "      <td>0.008179</td>\n",
              "      <td>7.974692e-03</td>\n",
              "      <td>7.836316e-03</td>\n",
              "      <td>7.985077e-03</td>\n",
              "      <td>8.453447e-03</td>\n",
              "      <td>8.543141e-03</td>\n",
              "      <td>7.968901e-03</td>\n",
              "      <td>7.787952e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.813627e-13</td>\n",
              "      <td>-1.941189e-13</td>\n",
              "      <td>-1.676032e-13</td>\n",
              "      <td>-3.610815e-14</td>\n",
              "      <td>8.697801e-14</td>\n",
              "      <td>1.488941e-13</td>\n",
              "      <td>3.876199e-14</td>\n",
              "      <td>1.113698e-13</td>\n",
              "      <td>3.546329e-13</td>\n",
              "      <td>-1.602816e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.929249e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>9.120397e-06</td>\n",
              "      <td>2.205470e-05</td>\n",
              "      <td>5.102658e-06</td>\n",
              "      <td>2.106591e-05</td>\n",
              "      <td>7.540939e-08</td>\n",
              "      <td>6.271493e-06</td>\n",
              "      <td>1.096370e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.476922e-07</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>3.806172e-05</td>\n",
              "      <td>3.835830e-07</td>\n",
              "      <td>6.750817e-06</td>\n",
              "      <td>1.863219e-05</td>\n",
              "      <td>1.346969e-05</td>\n",
              "      <td>5.569830e-06</td>\n",
              "      <td>-3.534751e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.550414e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>1.629342e-06</td>\n",
              "      <td>-1.235343e-06</td>\n",
              "      <td>8.224735e-07</td>\n",
              "      <td>-5.586875e-07</td>\n",
              "      <td>2.645753e-06</td>\n",
              "      <td>5.533508e-05</td>\n",
              "      <td>4.465951e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>4.530484e-04</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-2.018864e-04</td>\n",
              "      <td>-2.044283e-04</td>\n",
              "      <td>-1.777444e-04</td>\n",
              "      <td>-4.314623e-05</td>\n",
              "      <td>1.452080e-04</td>\n",
              "      <td>3.828391e-04</td>\n",
              "      <td>7.722454e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>8.580230e-04</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>3.330597e-04</td>\n",
              "      <td>1.518707e-04</td>\n",
              "      <td>1.039850e-04</td>\n",
              "      <td>7.744366e-05</td>\n",
              "      <td>1.735599e-05</td>\n",
              "      <td>-1.867246e-06</td>\n",
              "      <td>-5.283646e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>-1.072154e-10</td>\n",
              "      <td>-3.232369e-11</td>\n",
              "      <td>1.666647e-10</td>\n",
              "      <td>-3.448771e-10</td>\n",
              "      <td>5.652159e-10</td>\n",
              "      <td>-8.308132e-10</td>\n",
              "      <td>1.141762e-09</td>\n",
              "      <td>-1.497725e-09</td>\n",
              "      <td>1.894819e-09</td>\n",
              "      <td>-2.328016e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>3.091938e-02</td>\n",
              "      <td>0.030882</td>\n",
              "      <td>0.017899</td>\n",
              "      <td>2.623495e-02</td>\n",
              "      <td>1.683696e-02</td>\n",
              "      <td>-9.608166e-04</td>\n",
              "      <td>-1.694898e-02</td>\n",
              "      <td>-3.794178e-02</td>\n",
              "      <td>-4.874028e-02</td>\n",
              "      <td>-5.946823e-02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows  63945 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b61d2e02-b7fb-473f-a029-305cf0ea0f1a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b61d2e02-b7fb-473f-a029-305cf0ea0f1a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b61d2e02-b7fb-473f-a029-305cf0ea0f1a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.concat([dataset,audio],axis = 1)"
      ],
      "metadata": {
        "id": "Rb37lRHfTZrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "Oy8QMSxKThjn",
        "outputId": "cc54c443-5b4c-4f97-aed8-ca05ad1261f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Modality  Vocal channel  Emotion  Emotional intensity  Statement  \\\n",
              "0            3              1        5                    2          2   \n",
              "1            3              1        4                    2          2   \n",
              "2            3              1        1                    1          2   \n",
              "3            3              1        2                    2          2   \n",
              "4            3              1        4                    2          2   \n",
              "...        ...            ...      ...                  ...        ...   \n",
              "1435         3              1        8                    1          1   \n",
              "1436         3              1        5                    1          1   \n",
              "1437         3              1        2                    2          1   \n",
              "1438         3              1        4                    1          1   \n",
              "1439         3              1        6                    1          1   \n",
              "\n",
              "      Repetition  Actor             0             1             2  ...  \\\n",
              "0              1      1  2.329042e-04  1.441041e-03  3.753820e-04  ...   \n",
              "1              1      1  1.018356e-08 -1.005014e-08  9.524251e-09  ...   \n",
              "2              2      1  2.278331e-05  3.115162e-05  4.793048e-06  ...   \n",
              "3              1      1  2.429188e-05  2.923215e-05  8.048310e-06  ...   \n",
              "4              2      1  1.813627e-13 -1.941189e-13 -1.676032e-13  ...   \n",
              "...          ...    ...           ...           ...           ...  ...   \n",
              "1435           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1436           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1437           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1438           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1439           1     24 -1.072154e-10 -3.232369e-11  1.666647e-10  ...   \n",
              "\n",
              "             63935     63936     63937         63938         63939  \\\n",
              "0     4.532097e-02  0.050078  0.045070  3.029229e-02  3.539371e-02   \n",
              "1     3.179810e-05  0.000044  0.000026 -1.672583e-06  9.198144e-06   \n",
              "2     3.252479e-06  0.000013 -0.000002  3.341737e-07  5.308339e-07   \n",
              "3     8.280606e-03  0.008378  0.008179  7.974692e-03  7.836316e-03   \n",
              "4    -5.929249e-06  0.000005 -0.000005  9.120397e-06  2.205470e-05   \n",
              "...            ...       ...       ...           ...           ...   \n",
              "1435 -3.476922e-07 -0.000001  0.000004  3.806172e-05  3.835830e-07   \n",
              "1436 -2.550414e-06  0.000002 -0.000002  1.629342e-06 -1.235343e-06   \n",
              "1437  4.530484e-04  0.000148 -0.000035 -2.018864e-04 -2.044283e-04   \n",
              "1438  8.580230e-04  0.000592  0.000535  3.330597e-04  1.518707e-04   \n",
              "1439  3.091938e-02  0.030882  0.017899  2.623495e-02  1.683696e-02   \n",
              "\n",
              "             63940         63941         63942         63943         63944  \n",
              "0     5.646525e-02  5.664071e-02  5.869206e-02  5.510687e-02  6.350432e-02  \n",
              "1     2.301180e-05 -3.754093e-06  2.038091e-06 -1.250351e-06  7.715186e-07  \n",
              "2     1.392216e-05 -4.699850e-07 -2.686204e-08  2.657069e-07 -4.201772e-07  \n",
              "3     7.985077e-03  8.453447e-03  8.543141e-03  7.968901e-03  7.787952e-03  \n",
              "4     5.102658e-06  2.106591e-05  7.540939e-08  6.271493e-06  1.096370e-05  \n",
              "...            ...           ...           ...           ...           ...  \n",
              "1435  6.750817e-06  1.863219e-05  1.346969e-05  5.569830e-06 -3.534751e-06  \n",
              "1436  8.224735e-07 -5.586875e-07  2.645753e-06  5.533508e-05  4.465951e-05  \n",
              "1437 -1.777444e-04 -4.314623e-05  1.452080e-04  3.828391e-04  7.722454e-04  \n",
              "1438  1.039850e-04  7.744366e-05  1.735599e-05 -1.867246e-06 -5.283646e-06  \n",
              "1439 -9.608166e-04 -1.694898e-02 -3.794178e-02 -4.874028e-02 -5.946823e-02  \n",
              "\n",
              "[1440 rows x 63952 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ddaf9ea-cb9d-4f27-ba7e-9330bba6f33e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modality</th>\n",
              "      <th>Vocal channel</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotional intensity</th>\n",
              "      <th>Statement</th>\n",
              "      <th>Repetition</th>\n",
              "      <th>Actor</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>...</th>\n",
              "      <th>63935</th>\n",
              "      <th>63936</th>\n",
              "      <th>63937</th>\n",
              "      <th>63938</th>\n",
              "      <th>63939</th>\n",
              "      <th>63940</th>\n",
              "      <th>63941</th>\n",
              "      <th>63942</th>\n",
              "      <th>63943</th>\n",
              "      <th>63944</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.329042e-04</td>\n",
              "      <td>1.441041e-03</td>\n",
              "      <td>3.753820e-04</td>\n",
              "      <td>...</td>\n",
              "      <td>4.532097e-02</td>\n",
              "      <td>0.050078</td>\n",
              "      <td>0.045070</td>\n",
              "      <td>3.029229e-02</td>\n",
              "      <td>3.539371e-02</td>\n",
              "      <td>5.646525e-02</td>\n",
              "      <td>5.664071e-02</td>\n",
              "      <td>5.869206e-02</td>\n",
              "      <td>5.510687e-02</td>\n",
              "      <td>6.350432e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.018356e-08</td>\n",
              "      <td>-1.005014e-08</td>\n",
              "      <td>9.524251e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>3.179810e-05</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-1.672583e-06</td>\n",
              "      <td>9.198144e-06</td>\n",
              "      <td>2.301180e-05</td>\n",
              "      <td>-3.754093e-06</td>\n",
              "      <td>2.038091e-06</td>\n",
              "      <td>-1.250351e-06</td>\n",
              "      <td>7.715186e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2.278331e-05</td>\n",
              "      <td>3.115162e-05</td>\n",
              "      <td>4.793048e-06</td>\n",
              "      <td>...</td>\n",
              "      <td>3.252479e-06</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>3.341737e-07</td>\n",
              "      <td>5.308339e-07</td>\n",
              "      <td>1.392216e-05</td>\n",
              "      <td>-4.699850e-07</td>\n",
              "      <td>-2.686204e-08</td>\n",
              "      <td>2.657069e-07</td>\n",
              "      <td>-4.201772e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.429188e-05</td>\n",
              "      <td>2.923215e-05</td>\n",
              "      <td>8.048310e-06</td>\n",
              "      <td>...</td>\n",
              "      <td>8.280606e-03</td>\n",
              "      <td>0.008378</td>\n",
              "      <td>0.008179</td>\n",
              "      <td>7.974692e-03</td>\n",
              "      <td>7.836316e-03</td>\n",
              "      <td>7.985077e-03</td>\n",
              "      <td>8.453447e-03</td>\n",
              "      <td>8.543141e-03</td>\n",
              "      <td>7.968901e-03</td>\n",
              "      <td>7.787952e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.813627e-13</td>\n",
              "      <td>-1.941189e-13</td>\n",
              "      <td>-1.676032e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.929249e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>9.120397e-06</td>\n",
              "      <td>2.205470e-05</td>\n",
              "      <td>5.102658e-06</td>\n",
              "      <td>2.106591e-05</td>\n",
              "      <td>7.540939e-08</td>\n",
              "      <td>6.271493e-06</td>\n",
              "      <td>1.096370e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.476922e-07</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>3.806172e-05</td>\n",
              "      <td>3.835830e-07</td>\n",
              "      <td>6.750817e-06</td>\n",
              "      <td>1.863219e-05</td>\n",
              "      <td>1.346969e-05</td>\n",
              "      <td>5.569830e-06</td>\n",
              "      <td>-3.534751e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.550414e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>1.629342e-06</td>\n",
              "      <td>-1.235343e-06</td>\n",
              "      <td>8.224735e-07</td>\n",
              "      <td>-5.586875e-07</td>\n",
              "      <td>2.645753e-06</td>\n",
              "      <td>5.533508e-05</td>\n",
              "      <td>4.465951e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>4.530484e-04</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-2.018864e-04</td>\n",
              "      <td>-2.044283e-04</td>\n",
              "      <td>-1.777444e-04</td>\n",
              "      <td>-4.314623e-05</td>\n",
              "      <td>1.452080e-04</td>\n",
              "      <td>3.828391e-04</td>\n",
              "      <td>7.722454e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>8.580230e-04</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>3.330597e-04</td>\n",
              "      <td>1.518707e-04</td>\n",
              "      <td>1.039850e-04</td>\n",
              "      <td>7.744366e-05</td>\n",
              "      <td>1.735599e-05</td>\n",
              "      <td>-1.867246e-06</td>\n",
              "      <td>-5.283646e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>-1.072154e-10</td>\n",
              "      <td>-3.232369e-11</td>\n",
              "      <td>1.666647e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>3.091938e-02</td>\n",
              "      <td>0.030882</td>\n",
              "      <td>0.017899</td>\n",
              "      <td>2.623495e-02</td>\n",
              "      <td>1.683696e-02</td>\n",
              "      <td>-9.608166e-04</td>\n",
              "      <td>-1.694898e-02</td>\n",
              "      <td>-3.794178e-02</td>\n",
              "      <td>-4.874028e-02</td>\n",
              "      <td>-5.946823e-02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows  63952 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ddaf9ea-cb9d-4f27-ba7e-9330bba6f33e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ddaf9ea-cb9d-4f27-ba7e-9330bba6f33e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ddaf9ea-cb9d-4f27-ba7e-9330bba6f33e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_dataset.csv\",index = False)"
      ],
      "metadata": {
        "id": "rEWUN28fXC6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.iloc[:,7:] = np.abs(fft(dataset.iloc[:,7:],axis = 1))"
      ],
      "metadata": {
        "id": "B6dsT1e4XKui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_fft_dataset.csv\")"
      ],
      "metadata": {
        "id": "LS-RA37YYPEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.iloc[:,7:].values\n",
        "mel = []\n",
        "l = len(data)\n",
        "for i in range(l):\n",
        "  m = librosa.feature.melspectrogram(y = data[i,:])\n",
        "  mel.append(m.reshape((1,-1))[0])\n",
        "m = np.array(mel)"
      ],
      "metadata": {
        "id": "JVeRZvNLbeHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = pd.DataFrame(m)\n",
        "dataset = pd.concat([dataset.iloc[:,0:7],m],axis = 1)"
      ],
      "metadata": {
        "id": "sgKXDB3idGTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_mel_dataset.csv\",index = False)"
      ],
      "metadata": {
        "id": "Hl3Tdoseeizq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree"
      ],
      "metadata": {
        "id": "9-ZONjj1Le_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FFT Dataset"
      ],
      "metadata": {
        "id": "hwj5cEagMC72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_fft_dataset.csv\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "a97a0a91-7703-4d68-84ee-39caa6536251",
        "id": "bm2akBqJIW2L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0  Modality  Vocal channel  Emotion  Emotional intensity  \\\n",
              "0              0         3              1        5                    2   \n",
              "1              1         3              1        4                    2   \n",
              "2              2         3              1        1                    1   \n",
              "3              3         3              1        2                    2   \n",
              "4              4         3              1        4                    2   \n",
              "...          ...       ...            ...      ...                  ...   \n",
              "1435        1435         3              1        8                    1   \n",
              "1436        1436         3              1        5                    1   \n",
              "1437        1437         3              1        2                    2   \n",
              "1438        1438         3              1        4                    1   \n",
              "1439        1439         3              1        6                    1   \n",
              "\n",
              "      Statement  Repetition  Actor         0         1  ...     63935  \\\n",
              "0             2           1      1  0.562838  0.524753  ...  0.356059   \n",
              "1             2           1      1  0.044707  0.018661  ...  0.007676   \n",
              "2             2           2      1  0.065975  0.058365  ...  0.032877   \n",
              "3             2           1      1  0.019496  0.071197  ...  0.030344   \n",
              "4             2           2      1  0.065471  0.036018  ...  0.012859   \n",
              "...         ...         ...    ...       ...       ...  ...       ...   \n",
              "1435          1           2     24  0.009685  0.021843  ...  0.006263   \n",
              "1436          1           2     24  0.001113  0.006725  ...  0.031690   \n",
              "1437          1           2     24  0.196957  0.110510  ...  0.058543   \n",
              "1438          1           2     24  0.080306  0.058616  ...  0.066964   \n",
              "1439          1           1     24  0.008611  0.007329  ...  0.025166   \n",
              "\n",
              "         63936     63937     63938     63939     63940     63941     63942  \\\n",
              "0     0.279785  0.296763  0.210512  0.331878  0.300090  0.318311  0.209626   \n",
              "1     0.006029  0.004431  0.020777  0.026970  0.018854  0.006725  0.011850   \n",
              "2     0.041382  0.016992  0.031579  0.035822  0.024165  0.023249  0.021512   \n",
              "3     0.067316  0.020739  0.035819  0.065218  0.026406  0.055942  0.050371   \n",
              "4     0.019099  0.006354  0.018731  0.023008  0.013048  0.019262  0.009203   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1435  0.009038  0.011431  0.007916  0.042234  0.043470  0.014524  0.016566   \n",
              "1436  0.049714  0.025524  0.032055  0.029004  0.035755  0.030565  0.017503   \n",
              "1437  0.031835  0.085497  0.108549  0.029150  0.055502  0.083451  0.081203   \n",
              "1438  0.085966  0.049558  0.047485  0.017968  0.039558  0.026743  0.008940   \n",
              "1439  0.007609  0.009041  0.029945  0.035148  0.010279  0.028460  0.029625   \n",
              "\n",
              "         63943     63944  \n",
              "0     0.407262  0.524753  \n",
              "1     0.015253  0.018661  \n",
              "2     0.024077  0.058365  \n",
              "3     0.066295  0.071197  \n",
              "4     0.044080  0.036018  \n",
              "...        ...       ...  \n",
              "1435  0.005730  0.021843  \n",
              "1436  0.013667  0.006725  \n",
              "1437  0.010991  0.110510  \n",
              "1438  0.017256  0.058616  \n",
              "1439  0.011848  0.007329  \n",
              "\n",
              "[1440 rows x 63953 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11d6e779-64c8-44e7-8a46-9a2c496b3ca0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Modality</th>\n",
              "      <th>Vocal channel</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotional intensity</th>\n",
              "      <th>Statement</th>\n",
              "      <th>Repetition</th>\n",
              "      <th>Actor</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>...</th>\n",
              "      <th>63935</th>\n",
              "      <th>63936</th>\n",
              "      <th>63937</th>\n",
              "      <th>63938</th>\n",
              "      <th>63939</th>\n",
              "      <th>63940</th>\n",
              "      <th>63941</th>\n",
              "      <th>63942</th>\n",
              "      <th>63943</th>\n",
              "      <th>63944</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.562838</td>\n",
              "      <td>0.524753</td>\n",
              "      <td>...</td>\n",
              "      <td>0.356059</td>\n",
              "      <td>0.279785</td>\n",
              "      <td>0.296763</td>\n",
              "      <td>0.210512</td>\n",
              "      <td>0.331878</td>\n",
              "      <td>0.300090</td>\n",
              "      <td>0.318311</td>\n",
              "      <td>0.209626</td>\n",
              "      <td>0.407262</td>\n",
              "      <td>0.524753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.044707</td>\n",
              "      <td>0.018661</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007676</td>\n",
              "      <td>0.006029</td>\n",
              "      <td>0.004431</td>\n",
              "      <td>0.020777</td>\n",
              "      <td>0.026970</td>\n",
              "      <td>0.018854</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>0.011850</td>\n",
              "      <td>0.015253</td>\n",
              "      <td>0.018661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.065975</td>\n",
              "      <td>0.058365</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032877</td>\n",
              "      <td>0.041382</td>\n",
              "      <td>0.016992</td>\n",
              "      <td>0.031579</td>\n",
              "      <td>0.035822</td>\n",
              "      <td>0.024165</td>\n",
              "      <td>0.023249</td>\n",
              "      <td>0.021512</td>\n",
              "      <td>0.024077</td>\n",
              "      <td>0.058365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.019496</td>\n",
              "      <td>0.071197</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030344</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.020739</td>\n",
              "      <td>0.035819</td>\n",
              "      <td>0.065218</td>\n",
              "      <td>0.026406</td>\n",
              "      <td>0.055942</td>\n",
              "      <td>0.050371</td>\n",
              "      <td>0.066295</td>\n",
              "      <td>0.071197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.065471</td>\n",
              "      <td>0.036018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012859</td>\n",
              "      <td>0.019099</td>\n",
              "      <td>0.006354</td>\n",
              "      <td>0.018731</td>\n",
              "      <td>0.023008</td>\n",
              "      <td>0.013048</td>\n",
              "      <td>0.019262</td>\n",
              "      <td>0.009203</td>\n",
              "      <td>0.044080</td>\n",
              "      <td>0.036018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>1435</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.009685</td>\n",
              "      <td>0.021843</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.009038</td>\n",
              "      <td>0.011431</td>\n",
              "      <td>0.007916</td>\n",
              "      <td>0.042234</td>\n",
              "      <td>0.043470</td>\n",
              "      <td>0.014524</td>\n",
              "      <td>0.016566</td>\n",
              "      <td>0.005730</td>\n",
              "      <td>0.021843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>1436</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031690</td>\n",
              "      <td>0.049714</td>\n",
              "      <td>0.025524</td>\n",
              "      <td>0.032055</td>\n",
              "      <td>0.029004</td>\n",
              "      <td>0.035755</td>\n",
              "      <td>0.030565</td>\n",
              "      <td>0.017503</td>\n",
              "      <td>0.013667</td>\n",
              "      <td>0.006725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>1437</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.196957</td>\n",
              "      <td>0.110510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.058543</td>\n",
              "      <td>0.031835</td>\n",
              "      <td>0.085497</td>\n",
              "      <td>0.108549</td>\n",
              "      <td>0.029150</td>\n",
              "      <td>0.055502</td>\n",
              "      <td>0.083451</td>\n",
              "      <td>0.081203</td>\n",
              "      <td>0.010991</td>\n",
              "      <td>0.110510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>1438</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.080306</td>\n",
              "      <td>0.058616</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066964</td>\n",
              "      <td>0.085966</td>\n",
              "      <td>0.049558</td>\n",
              "      <td>0.047485</td>\n",
              "      <td>0.017968</td>\n",
              "      <td>0.039558</td>\n",
              "      <td>0.026743</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>0.017256</td>\n",
              "      <td>0.058616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>1439</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>0.008611</td>\n",
              "      <td>0.007329</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025166</td>\n",
              "      <td>0.007609</td>\n",
              "      <td>0.009041</td>\n",
              "      <td>0.029945</td>\n",
              "      <td>0.035148</td>\n",
              "      <td>0.010279</td>\n",
              "      <td>0.028460</td>\n",
              "      <td>0.029625</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.007329</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows  63953 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11d6e779-64c8-44e7-8a46-9a2c496b3ca0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11d6e779-64c8-44e7-8a46-9a2c496b3ca0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11d6e779-64c8-44e7-8a46-9a2c496b3ca0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying PCA and LDA"
      ],
      "metadata": {
        "id": "uzp9qpQwMHj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 500)\n",
        "lda = LDA(n_components = 7)"
      ],
      "metadata": {
        "id": "cERRBddCIW2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataset[\"Emotion\"]\n",
        "X = dataset.iloc[:,7:]"
      ],
      "metadata": {
        "id": "3DBslfD9IW2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = pca.fit(X)\n",
        "lda = lda.fit(X,Y)\n",
        "X_pca = pca.transform(X)\n",
        "X_lda = lda.transform(X)"
      ],
      "metadata": {
        "id": "cgqdUDB_IW2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset"
      ],
      "metadata": {
        "id": "Dox4yMJAMLkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_pca,tv_X_pca,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_pca,valid_X_pca,test_Y,valid_Y = train_test_split(tv_X_pca,tv_Y,random_state = 12345,train_size = 0.5)\n",
        "train_X_lda,tv_X_lda,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_lda,valid_X_lda,test_Y,valid_Y = train_test_split(tv_X_lda,tv_Y,random_state = 12345,train_size = 0.5)\n"
      ],
      "metadata": {
        "id": "Jo-I-d1IIW2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_gi_pca,ss_gi_pca = best_hp('gini',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "d_en_pca,ss_en_pca = best_hp('entropy',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "d_ll_pca,ss_ll_pca = best_hp('log_loss',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "\n",
        "d_gi_lda,ss_gi_lda = best_hp('gini',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "d_en_lda,ss_en_lda = best_hp('entropy',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "d_ll_lda,ss_ll_lda = best_hp('log_loss',train_X_lda,train_Y,valid_X_lda,valid_Y)"
      ],
      "metadata": {
        "id": "Qao2CIVAIW2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_gi_pca = DecisionTreeClassifier(criterion='gini',max_depth=d_gi_pca,min_samples_split=ss_gi_pca).fit(train_X_pca,train_Y)\n",
        "tree_en_pca = DecisionTreeClassifier(criterion='entropy',max_depth=d_en_pca,min_samples_split=ss_en_pca).fit(train_X_pca,train_Y)\n",
        "tree_ll_pca = DecisionTreeClassifier(criterion='log_loss',max_depth=d_ll_pca,min_samples_split=ss_ll_pca).fit(train_X_pca,train_Y)\n",
        "\n",
        "tree_gi_lda = DecisionTreeClassifier(criterion='gini',max_depth=d_gi_lda,min_samples_split=ss_gi_lda).fit(train_X_lda,train_Y)\n",
        "tree_en_lda = DecisionTreeClassifier(criterion='entropy',max_depth=d_en_lda,min_samples_split=ss_en_lda).fit(train_X_lda,train_Y)\n",
        "tree_ll_lda = DecisionTreeClassifier(criterion='log_loss',max_depth=d_ll_lda,min_samples_split=ss_ll_lda).fit(train_X_lda,train_Y)\n",
        "\n"
      ],
      "metadata": {
        "id": "4BPK0mQrIW2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "c2C2IGp0MOvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clk = [tree_gi_pca,tree_en_pca,tree_ll_pca,tree_gi_lda,tree_en_lda,tree_ll_lda]\n",
        "clk_name = ['tree_gi_pca','tree_en_pca','tree_ll_pca','tree_gi_lda','tree_en_lda','tree_ll_lda']\n",
        "l = len(clk)\n",
        "for i in range(l):\n",
        "  print(clk_name[i])\n",
        "  if i < 3:\n",
        "    test_X = test_X_pca\n",
        "  else:\n",
        "    test_X = test_X_lda\n",
        "  print(classification_report(test_Y,clk[i].predict(test_X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ad06b-5fdb-490e-b507-2f9acdf84287",
        "id": "g_Nurq4NIW2O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree_gi_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.06      0.14      0.08        14\n",
            "           2       0.33      0.35      0.34        34\n",
            "           3       0.27      0.19      0.22        42\n",
            "           4       0.23      0.19      0.21        43\n",
            "           5       0.43      0.28      0.34        32\n",
            "           6       0.34      0.31      0.32        39\n",
            "           7       0.31      0.38      0.34        37\n",
            "           8       0.35      0.38      0.37        47\n",
            "\n",
            "    accuracy                           0.29       288\n",
            "   macro avg       0.29      0.28      0.28       288\n",
            "weighted avg       0.31      0.29      0.29       288\n",
            "\n",
            "tree_en_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.33      0.76      0.46        34\n",
            "           3       0.20      0.83      0.32        42\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.48      0.50      0.49        32\n",
            "           6       0.00      0.00      0.00        39\n",
            "           7       0.00      0.00      0.00        37\n",
            "           8       0.00      0.00      0.00        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.13      0.26      0.16       288\n",
            "weighted avg       0.12      0.27      0.16       288\n",
            "\n",
            "tree_ll_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.33      0.76      0.46        34\n",
            "           3       0.20      0.83      0.32        42\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.48      0.50      0.49        32\n",
            "           6       0.00      0.00      0.00        39\n",
            "           7       0.00      0.00      0.00        37\n",
            "           8       0.00      0.00      0.00        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.13      0.26      0.16       288\n",
            "weighted avg       0.12      0.27      0.16       288\n",
            "\n",
            "tree_gi_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.06      0.14      0.08        14\n",
            "           2       0.33      0.35      0.34        34\n",
            "           3       0.27      0.19      0.22        42\n",
            "           4       0.23      0.19      0.21        43\n",
            "           5       0.41      0.28      0.33        32\n",
            "           6       0.32      0.28      0.30        39\n",
            "           7       0.31      0.38      0.34        37\n",
            "           8       0.35      0.38      0.37        47\n",
            "\n",
            "    accuracy                           0.28       288\n",
            "   macro avg       0.29      0.27      0.27       288\n",
            "weighted avg       0.30      0.28      0.29       288\n",
            "\n",
            "tree_en_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.33      0.76      0.46        34\n",
            "           3       0.20      0.83      0.32        42\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.48      0.50      0.49        32\n",
            "           6       0.00      0.00      0.00        39\n",
            "           7       0.00      0.00      0.00        37\n",
            "           8       0.00      0.00      0.00        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.13      0.26      0.16       288\n",
            "weighted avg       0.12      0.27      0.16       288\n",
            "\n",
            "tree_ll_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.33      0.76      0.46        34\n",
            "           3       0.20      0.83      0.32        42\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.48      0.50      0.49        32\n",
            "           6       0.00      0.00      0.00        39\n",
            "           7       0.00      0.00      0.00        37\n",
            "           8       0.00      0.00      0.00        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.13      0.26      0.16       288\n",
            "weighted avg       0.12      0.27      0.16       288\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melspectogram Dataset"
      ],
      "metadata": {
        "id": "jllpl4KAMz06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_mel_dataset.csv\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "7dda74a2-1012-45e8-b4ee-b089cbbde546",
        "id": "cKUxRcAEIYQq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Modality  Vocal channel  Emotion  Emotional intensity  Statement  \\\n",
              "0            3              1        5                    2          2   \n",
              "1            3              1        4                    2          2   \n",
              "2            3              1        1                    1          2   \n",
              "3            3              1        2                    2          2   \n",
              "4            3              1        4                    2          2   \n",
              "...        ...            ...      ...                  ...        ...   \n",
              "1435         3              1        8                    1          1   \n",
              "1436         3              1        5                    1          1   \n",
              "1437         3              1        2                    2          1   \n",
              "1438         3              1        4                    1          1   \n",
              "1439         3              1        6                    1          1   \n",
              "\n",
              "      Repetition  Actor             0             1             2  ...  \\\n",
              "0              1      1  7.569644e-07  5.010802e-06  8.384924e-06  ...   \n",
              "1              1      1  2.794822e-10  1.450041e-10  1.175133e-10  ...   \n",
              "2              2      1  1.332402e-07  1.478175e-07  1.084568e-07  ...   \n",
              "3              1      1  1.523058e-06  1.167175e-06  2.473346e-07  ...   \n",
              "4              2      1  1.922096e-10  3.086805e-10  1.442073e-10  ...   \n",
              "...          ...    ...           ...           ...           ...  ...   \n",
              "1435           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1436           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1437           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1438           2     24  0.000000e+00  0.000000e+00  0.000000e+00  ...   \n",
              "1439           1     24  5.361655e-11  6.365203e-11  3.540531e-12  ...   \n",
              "\n",
              "             15990         15991         15992         15993         15994  \\\n",
              "0     5.499699e-05  5.885491e-06  1.657120e-05  7.602131e-05  6.069553e-05   \n",
              "1     9.339636e-10  1.220944e-09  3.162984e-09  2.801177e-09  1.114704e-09   \n",
              "2     1.416472e-10  7.174033e-11  1.259040e-10  1.607572e-10  3.343664e-11   \n",
              "3     3.866556e-08  9.828029e-08  1.197246e-07  8.062022e-08  3.255798e-08   \n",
              "4     6.283513e-09  3.751002e-09  1.515500e-09  1.571249e-09  1.424604e-09   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "1435  9.985296e-10  4.968911e-10  3.221174e-10  3.396013e-10  3.091778e-10   \n",
              "1436  6.965746e-08  6.391710e-08  5.344007e-08  5.219970e-08  1.727351e-08   \n",
              "1437  2.448344e-08  8.441065e-09  8.191249e-08  2.261750e-07  1.162307e-07   \n",
              "1438  6.350594e-08  1.018883e-07  1.052788e-07  5.846529e-08  3.531843e-08   \n",
              "1439  5.118569e-06  4.369101e-06  1.032162e-06  9.775675e-08  6.991492e-08   \n",
              "\n",
              "             15995         15996         15997         15998         15999  \n",
              "0     2.774049e-05  1.739454e-05  3.164505e-05  3.757783e-05  7.998535e-05  \n",
              "1     4.183812e-10  3.350986e-10  3.824072e-10  3.206519e-10  2.361991e-10  \n",
              "2     5.016731e-11  4.086213e-11  3.167913e-11  1.258924e-10  2.130951e-10  \n",
              "3     2.372752e-09  6.229968e-09  3.475527e-08  3.888261e-08  5.073903e-07  \n",
              "4     1.195403e-09  5.786857e-10  5.123155e-10  4.680149e-10  3.469635e-10  \n",
              "...            ...           ...           ...           ...           ...  \n",
              "1435  3.297025e-10  4.210789e-10  4.688974e-10  7.026459e-10  5.663512e-10  \n",
              "1436  7.570125e-09  3.529409e-09  1.575603e-09  1.517090e-09  1.485708e-09  \n",
              "1437  5.961151e-08  5.394006e-08  5.558606e-08  3.873275e-08  4.627699e-08  \n",
              "1438  2.501399e-08  2.186149e-08  2.132620e-08  1.741378e-08  1.488148e-08  \n",
              "1439  6.688761e-08  2.848459e-07  1.374716e-06  1.566018e-06  3.060645e-05  \n",
              "\n",
              "[1440 rows x 16007 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2497578-f66f-4188-ab5d-a660090503fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modality</th>\n",
              "      <th>Vocal channel</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotional intensity</th>\n",
              "      <th>Statement</th>\n",
              "      <th>Repetition</th>\n",
              "      <th>Actor</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>...</th>\n",
              "      <th>15990</th>\n",
              "      <th>15991</th>\n",
              "      <th>15992</th>\n",
              "      <th>15993</th>\n",
              "      <th>15994</th>\n",
              "      <th>15995</th>\n",
              "      <th>15996</th>\n",
              "      <th>15997</th>\n",
              "      <th>15998</th>\n",
              "      <th>15999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.569644e-07</td>\n",
              "      <td>5.010802e-06</td>\n",
              "      <td>8.384924e-06</td>\n",
              "      <td>...</td>\n",
              "      <td>5.499699e-05</td>\n",
              "      <td>5.885491e-06</td>\n",
              "      <td>1.657120e-05</td>\n",
              "      <td>7.602131e-05</td>\n",
              "      <td>6.069553e-05</td>\n",
              "      <td>2.774049e-05</td>\n",
              "      <td>1.739454e-05</td>\n",
              "      <td>3.164505e-05</td>\n",
              "      <td>3.757783e-05</td>\n",
              "      <td>7.998535e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.794822e-10</td>\n",
              "      <td>1.450041e-10</td>\n",
              "      <td>1.175133e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>9.339636e-10</td>\n",
              "      <td>1.220944e-09</td>\n",
              "      <td>3.162984e-09</td>\n",
              "      <td>2.801177e-09</td>\n",
              "      <td>1.114704e-09</td>\n",
              "      <td>4.183812e-10</td>\n",
              "      <td>3.350986e-10</td>\n",
              "      <td>3.824072e-10</td>\n",
              "      <td>3.206519e-10</td>\n",
              "      <td>2.361991e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.332402e-07</td>\n",
              "      <td>1.478175e-07</td>\n",
              "      <td>1.084568e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>1.416472e-10</td>\n",
              "      <td>7.174033e-11</td>\n",
              "      <td>1.259040e-10</td>\n",
              "      <td>1.607572e-10</td>\n",
              "      <td>3.343664e-11</td>\n",
              "      <td>5.016731e-11</td>\n",
              "      <td>4.086213e-11</td>\n",
              "      <td>3.167913e-11</td>\n",
              "      <td>1.258924e-10</td>\n",
              "      <td>2.130951e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.523058e-06</td>\n",
              "      <td>1.167175e-06</td>\n",
              "      <td>2.473346e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>3.866556e-08</td>\n",
              "      <td>9.828029e-08</td>\n",
              "      <td>1.197246e-07</td>\n",
              "      <td>8.062022e-08</td>\n",
              "      <td>3.255798e-08</td>\n",
              "      <td>2.372752e-09</td>\n",
              "      <td>6.229968e-09</td>\n",
              "      <td>3.475527e-08</td>\n",
              "      <td>3.888261e-08</td>\n",
              "      <td>5.073903e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.922096e-10</td>\n",
              "      <td>3.086805e-10</td>\n",
              "      <td>1.442073e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>6.283513e-09</td>\n",
              "      <td>3.751002e-09</td>\n",
              "      <td>1.515500e-09</td>\n",
              "      <td>1.571249e-09</td>\n",
              "      <td>1.424604e-09</td>\n",
              "      <td>1.195403e-09</td>\n",
              "      <td>5.786857e-10</td>\n",
              "      <td>5.123155e-10</td>\n",
              "      <td>4.680149e-10</td>\n",
              "      <td>3.469635e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>9.985296e-10</td>\n",
              "      <td>4.968911e-10</td>\n",
              "      <td>3.221174e-10</td>\n",
              "      <td>3.396013e-10</td>\n",
              "      <td>3.091778e-10</td>\n",
              "      <td>3.297025e-10</td>\n",
              "      <td>4.210789e-10</td>\n",
              "      <td>4.688974e-10</td>\n",
              "      <td>7.026459e-10</td>\n",
              "      <td>5.663512e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>6.965746e-08</td>\n",
              "      <td>6.391710e-08</td>\n",
              "      <td>5.344007e-08</td>\n",
              "      <td>5.219970e-08</td>\n",
              "      <td>1.727351e-08</td>\n",
              "      <td>7.570125e-09</td>\n",
              "      <td>3.529409e-09</td>\n",
              "      <td>1.575603e-09</td>\n",
              "      <td>1.517090e-09</td>\n",
              "      <td>1.485708e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>2.448344e-08</td>\n",
              "      <td>8.441065e-09</td>\n",
              "      <td>8.191249e-08</td>\n",
              "      <td>2.261750e-07</td>\n",
              "      <td>1.162307e-07</td>\n",
              "      <td>5.961151e-08</td>\n",
              "      <td>5.394006e-08</td>\n",
              "      <td>5.558606e-08</td>\n",
              "      <td>3.873275e-08</td>\n",
              "      <td>4.627699e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>6.350594e-08</td>\n",
              "      <td>1.018883e-07</td>\n",
              "      <td>1.052788e-07</td>\n",
              "      <td>5.846529e-08</td>\n",
              "      <td>3.531843e-08</td>\n",
              "      <td>2.501399e-08</td>\n",
              "      <td>2.186149e-08</td>\n",
              "      <td>2.132620e-08</td>\n",
              "      <td>1.741378e-08</td>\n",
              "      <td>1.488148e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>5.361655e-11</td>\n",
              "      <td>6.365203e-11</td>\n",
              "      <td>3.540531e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>5.118569e-06</td>\n",
              "      <td>4.369101e-06</td>\n",
              "      <td>1.032162e-06</td>\n",
              "      <td>9.775675e-08</td>\n",
              "      <td>6.991492e-08</td>\n",
              "      <td>6.688761e-08</td>\n",
              "      <td>2.848459e-07</td>\n",
              "      <td>1.374716e-06</td>\n",
              "      <td>1.566018e-06</td>\n",
              "      <td>3.060645e-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows  16007 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2497578-f66f-4188-ab5d-a660090503fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2497578-f66f-4188-ab5d-a660090503fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2497578-f66f-4188-ab5d-a660090503fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying PCA and LDA"
      ],
      "metadata": {
        "id": "T5vKcTEsNAfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 500)\n",
        "lda = LDA(n_components = 7)"
      ],
      "metadata": {
        "id": "aQe9ASVgIYQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataset[\"Emotion\"]\n",
        "X = dataset.iloc[:,7:]"
      ],
      "metadata": {
        "id": "TdiO5R4GIYQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = pca.fit(X)\n",
        "lda = lda.fit(X,Y)\n",
        "X_pca = pca.transform(X)\n",
        "X_lda = lda.transform(X)"
      ],
      "metadata": {
        "id": "em-9sttWIYQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_pca,tv_X_pca,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_pca,valid_X_pca,test_Y,valid_Y = train_test_split(tv_X_pca,tv_Y,random_state = 12345,train_size = 0.5)\n",
        "train_X_lda,tv_X_lda,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_lda,valid_X_lda,test_Y,valid_Y = train_test_split(tv_X_lda,tv_Y,random_state = 12345,train_size = 0.5)\n"
      ],
      "metadata": {
        "id": "FqBkkcFcIYQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_gi_pca,ss_gi_pca = best_hp('gini',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "d_en_pca,ss_en_pca = best_hp('entropy',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "d_ll_pca,ss_ll_pca = best_hp('log_loss',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "\n",
        "d_gi_lda,ss_gi_lda = best_hp('gini',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "d_en_lda,ss_en_lda = best_hp('entropy',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "d_ll_lda,ss_ll_lda = best_hp('log_loss',train_X_lda,train_Y,valid_X_lda,valid_Y)"
      ],
      "metadata": {
        "id": "L-RoIZdUIYQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_gi_pca = DecisionTreeClassifier(criterion='gini',max_depth=d_gi_pca,min_samples_split=ss_gi_pca).fit(train_X_pca,train_Y)\n",
        "tree_en_pca = DecisionTreeClassifier(criterion='entropy',max_depth=d_en_pca,min_samples_split=ss_en_pca).fit(train_X_pca,train_Y)\n",
        "tree_ll_pca = DecisionTreeClassifier(criterion='log_loss',max_depth=d_ll_pca,min_samples_split=ss_ll_pca).fit(train_X_pca,train_Y)\n",
        "\n",
        "tree_gi_lda = DecisionTreeClassifier(criterion='gini',max_depth=d_gi_lda,min_samples_split=ss_gi_lda).fit(train_X_lda,train_Y)\n",
        "tree_en_lda = DecisionTreeClassifier(criterion='entropy',max_depth=d_en_lda,min_samples_split=ss_en_lda).fit(train_X_lda,train_Y)\n",
        "tree_ll_lda = DecisionTreeClassifier(criterion='log_loss',max_depth=d_ll_lda,min_samples_split=ss_ll_lda).fit(train_X_lda,train_Y)\n",
        "\n"
      ],
      "metadata": {
        "id": "SSaDg96kIYQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result"
      ],
      "metadata": {
        "id": "voTF3csGO5jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clk = [tree_gi_pca,tree_en_pca,tree_ll_pca,tree_gi_lda,tree_en_lda,tree_ll_lda]\n",
        "clk_name = ['tree_gi_pca','tree_en_pca','tree_ll_pca','tree_gi_lda','tree_en_lda','tree_ll_lda']\n",
        "l = len(clk)\n",
        "for i in range(l):\n",
        "  print(clk_name[i])\n",
        "  if i < 3:\n",
        "    test_X = test_X_pca\n",
        "  else:\n",
        "    test_X = test_X_lda\n",
        "  print(classification_report(test_Y,clk[i].predict(test_X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a79d79-51cd-4faa-c700-15e35afee92a",
        "id": "3YFtKKNbIYQt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree_gi_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.34      0.68      0.46        34\n",
            "           3       0.00      0.00      0.00        42\n",
            "           4       0.23      0.26      0.24        43\n",
            "           5       0.48      0.41      0.44        32\n",
            "           6       0.00      0.00      0.00        39\n",
            "           7       0.20      0.38      0.26        37\n",
            "           8       0.29      0.47      0.35        47\n",
            "\n",
            "    accuracy                           0.29       288\n",
            "   macro avg       0.19      0.27      0.22       288\n",
            "weighted avg       0.20      0.29      0.23       288\n",
            "\n",
            "tree_en_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.44      0.50      0.47        14\n",
            "           2       0.28      0.65      0.39        34\n",
            "           3       0.10      0.02      0.04        42\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.30      0.41      0.35        32\n",
            "           6       0.20      0.03      0.05        39\n",
            "           7       0.22      0.30      0.25        37\n",
            "           8       0.31      0.49      0.38        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.23      0.30      0.24       288\n",
            "weighted avg       0.21      0.27      0.21       288\n",
            "\n",
            "tree_ll_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.22      0.43      0.29        14\n",
            "           2       0.37      0.32      0.34        34\n",
            "           3       0.21      0.26      0.23        42\n",
            "           4       0.24      0.28      0.26        43\n",
            "           5       0.29      0.38      0.32        32\n",
            "           6       0.15      0.08      0.10        39\n",
            "           7       0.19      0.16      0.18        37\n",
            "           8       0.37      0.28      0.32        47\n",
            "\n",
            "    accuracy                           0.26       288\n",
            "   macro avg       0.25      0.27      0.26       288\n",
            "weighted avg       0.26      0.26      0.25       288\n",
            "\n",
            "tree_gi_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.34      0.68      0.46        34\n",
            "           3       0.00      0.00      0.00        42\n",
            "           4       0.23      0.26      0.24        43\n",
            "           5       0.48      0.41      0.44        32\n",
            "           6       0.00      0.00      0.00        39\n",
            "           7       0.20      0.38      0.26        37\n",
            "           8       0.29      0.47      0.35        47\n",
            "\n",
            "    accuracy                           0.29       288\n",
            "   macro avg       0.19      0.27      0.22       288\n",
            "weighted avg       0.20      0.29      0.23       288\n",
            "\n",
            "tree_en_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.20      0.14      0.17        14\n",
            "           2       0.27      0.26      0.27        34\n",
            "           3       0.19      0.26      0.22        42\n",
            "           4       0.23      0.26      0.24        43\n",
            "           5       0.31      0.44      0.36        32\n",
            "           6       0.10      0.05      0.07        39\n",
            "           7       0.16      0.19      0.18        37\n",
            "           8       0.45      0.30      0.36        47\n",
            "\n",
            "    accuracy                           0.24       288\n",
            "   macro avg       0.24      0.24      0.23       288\n",
            "weighted avg       0.25      0.24      0.24       288\n",
            "\n",
            "tree_ll_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.44      0.50      0.47        14\n",
            "           2       0.28      0.65      0.39        34\n",
            "           3       0.10      0.02      0.04        42\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.30      0.41      0.35        32\n",
            "           6       0.20      0.03      0.05        39\n",
            "           7       0.22      0.30      0.25        37\n",
            "           8       0.31      0.49      0.38        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.23      0.30      0.24       288\n",
            "weighted avg       0.21      0.27      0.21       288\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaboost"
      ],
      "metadata": {
        "id": "V3YeUVKnO_iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FFT dataset"
      ],
      "metadata": {
        "id": "Z38yK7GzQ_vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_fft_dataset.csv\")"
      ],
      "metadata": {
        "id": "4_vDPKL4hln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying PCA and LDA"
      ],
      "metadata": {
        "id": "KKdqCV6tRGzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 500)\n",
        "lda = LDA(n_components = 7)"
      ],
      "metadata": {
        "id": "FC63cWbxhln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataset[\"Emotion\"]\n",
        "X = dataset.iloc[:,7:]"
      ],
      "metadata": {
        "id": "eqGc0FKChln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pca = pca.fit(X)\n",
        "lda = lda.fit(X,Y)\n",
        "X_pca = pca.transform(X)\n",
        "X_lda = lda.transform(X)"
      ],
      "metadata": {
        "id": "owdwD7Tehln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_pca,tv_X_pca,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_pca,valid_X_pca,test_Y,valid_Y = train_test_split(tv_X_pca,tv_Y,random_state = 12345,train_size = 0.5)\n",
        "train_X_lda,tv_X_lda,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_lda,valid_X_lda,test_Y,valid_Y = train_test_split(tv_X_lda,tv_Y,random_state = 12345,train_size = 0.5)\n"
      ],
      "metadata": {
        "id": "5G2diL1Bhln6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kgip = best_para('gini',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kenp = best_para('entropy',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kllp = best_para('log_loss',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kgil = best_para('gini',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "kenl = best_para('entropy',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "klll = best_para('log_loss',train_X_lda,train_Y,valid_X_lda,valid_Y)"
      ],
      "metadata": {
        "id": "qXjtVy4Chln6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "951WMlDIRt0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_gi_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = 7),n_estimators=kgip).fit(train_X_pca,train_Y)\n",
        "ada_en_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = 7),n_estimators=kenp).fit(train_X_pca,train_Y)\n",
        "ada_ll_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = 7),n_estimators=kllp).fit(train_X_pca,train_Y)\n",
        "ada_gi_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = 7),n_estimators=kgil).fit(train_X_lda,train_Y)\n",
        "ada_en_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = 7),n_estimators=kenl).fit(train_X_lda,train_Y)\n",
        "ada_ll_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = 7),n_estimators=klll).fit(train_X_lda,train_Y)\n",
        "\n",
        "\n",
        "clk = [ada_gi_pca,ada_en_pca,ada_ll_pca,ada_gi_lda,ada_en_lda,ada_ll_lda]\n",
        "clk_name = ['ada_gi_pca','ada_en_pca','ada_ll_pca','ada_gi_lda','ada_en_lda','ada_ll_lda']\n",
        "l = len(clk)\n",
        "for i in range(l):\n",
        "  print(clk_name[i])\n",
        "  if i < 3:\n",
        "    test_X = test_X_pca\n",
        "  else:\n",
        "    test_X = test_X_lda\n",
        "  print(classification_report(test_Y,clk[i].predict(test_X)))"
      ],
      "metadata": {
        "id": "J4Zni6Nahln6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd8d691-d627-4e2e-bba4-1712c5fbb4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ada_gi_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.34      0.62      0.44        34\n",
            "           3       0.19      0.17      0.18        42\n",
            "           4       0.32      0.28      0.30        43\n",
            "           5       0.33      0.34      0.34        32\n",
            "           6       0.25      0.18      0.21        39\n",
            "           7       0.12      0.19      0.15        37\n",
            "           8       0.28      0.17      0.21        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.23      0.24      0.23       288\n",
            "weighted avg       0.25      0.25      0.24       288\n",
            "\n",
            "ada_en_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.50      0.14      0.22        14\n",
            "           2       0.37      0.71      0.48        34\n",
            "           3       0.19      0.17      0.18        42\n",
            "           4       0.20      0.09      0.13        43\n",
            "           5       0.35      0.47      0.40        32\n",
            "           6       0.19      0.18      0.18        39\n",
            "           7       0.21      0.27      0.24        37\n",
            "           8       0.26      0.19      0.22        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.28      0.28      0.26       288\n",
            "weighted avg       0.26      0.27      0.25       288\n",
            "\n",
            "ada_ll_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.37      0.65      0.47        34\n",
            "           3       0.22      0.19      0.21        42\n",
            "           4       0.19      0.14      0.16        43\n",
            "           5       0.33      0.53      0.41        32\n",
            "           6       0.22      0.13      0.16        39\n",
            "           7       0.14      0.16      0.15        37\n",
            "           8       0.35      0.32      0.33        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.23      0.26      0.24       288\n",
            "weighted avg       0.25      0.27      0.25       288\n",
            "\n",
            "ada_gi_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.17      0.07      0.10        14\n",
            "           2       0.32      0.62      0.42        34\n",
            "           3       0.06      0.05      0.05        42\n",
            "           4       0.17      0.12      0.14        43\n",
            "           5       0.35      0.44      0.39        32\n",
            "           6       0.32      0.18      0.23        39\n",
            "           7       0.14      0.22      0.17        37\n",
            "           8       0.24      0.19      0.21        47\n",
            "\n",
            "    accuracy                           0.23       288\n",
            "   macro avg       0.22      0.23      0.21       288\n",
            "weighted avg       0.22      0.23      0.21       288\n",
            "\n",
            "ada_en_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.33      0.07      0.12        14\n",
            "           2       0.41      0.71      0.52        34\n",
            "           3       0.20      0.12      0.15        42\n",
            "           4       0.23      0.14      0.17        43\n",
            "           5       0.27      0.31      0.29        32\n",
            "           6       0.29      0.31      0.30        39\n",
            "           7       0.14      0.19      0.16        37\n",
            "           8       0.24      0.23      0.24        47\n",
            "\n",
            "    accuracy                           0.26       288\n",
            "   macro avg       0.26      0.26      0.24       288\n",
            "weighted avg       0.25      0.26      0.25       288\n",
            "\n",
            "ada_ll_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.33      0.68      0.44        34\n",
            "           3       0.19      0.12      0.14        42\n",
            "           4       0.21      0.12      0.15        43\n",
            "           5       0.38      0.50      0.43        32\n",
            "           6       0.25      0.18      0.21        39\n",
            "           7       0.21      0.27      0.24        37\n",
            "           8       0.32      0.34      0.33        47\n",
            "\n",
            "    accuracy                           0.28       288\n",
            "   macro avg       0.24      0.28      0.24       288\n",
            "weighted avg       0.25      0.28      0.26       288\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kgip,dgip,sgip = best_para_with_hp('gini',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kenp,denp,senp = best_para_with_hp('entropy',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kllp,dllp,sllp = best_para_with_hp('log_loss',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kgil,dgil,sgil = best_para_with_hp('gini',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "kenl,denl,senl = best_para_with_hp('entropy',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "klll,dlll,slll = best_para_with_hp('log_loss',train_X_lda,train_Y,valid_X_lda,valid_Y)"
      ],
      "metadata": {
        "id": "Djv9H9D1hln6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "t5R9xJlORwaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_gi_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = dgip,min_samples_split=sgip),n_estimators=kgip).fit(train_X_pca,train_Y)\n",
        "ada_en_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = denp,min_samples_split=senp),n_estimators=kenp).fit(train_X_pca,train_Y)\n",
        "ada_ll_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = dllp,min_samples_split=sllp),n_estimators=kllp).fit(train_X_pca,train_Y)\n",
        "ada_gi_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = dgil,min_samples_split=sgil),n_estimators=kgil).fit(train_X_lda,train_Y)\n",
        "ada_en_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = denl,min_samples_split=senl),n_estimators=kenl).fit(train_X_lda,train_Y)\n",
        "ada_ll_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = dlll,min_samples_split=slll),n_estimators=klll).fit(train_X_lda,train_Y)\n",
        "\n",
        "\n",
        "clk = [ada_gi_pca,ada_en_pca,ada_ll_pca,ada_gi_lda,ada_en_lda,ada_ll_lda]\n",
        "clk_name = ['ada_gi_pca','ada_en_pca','ada_ll_pca','ada_gi_lda','ada_en_lda','ada_ll_lda']\n",
        "l = len(clk)\n",
        "for i in range(l):\n",
        "  print(clk_name[i])\n",
        "  if i < 3:\n",
        "    test_X = test_X_pca\n",
        "  else:\n",
        "    test_X = test_X_lda\n",
        "  print(classification_report(test_Y,clk[i].predict(test_X)))"
      ],
      "metadata": {
        "id": "8fk_nnDvhln6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3760e0dc-7e2b-4a58-ed8e-c27c9bc59916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ada_gi_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.05      0.07      0.06        14\n",
            "           2       0.37      0.41      0.39        34\n",
            "           3       0.21      0.21      0.21        42\n",
            "           4       0.19      0.14      0.16        43\n",
            "           5       0.38      0.56      0.46        32\n",
            "           6       0.22      0.15      0.18        39\n",
            "           7       0.17      0.24      0.20        37\n",
            "           8       0.31      0.19      0.24        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.24      0.25      0.24       288\n",
            "weighted avg       0.25      0.25      0.24       288\n",
            "\n",
            "ada_en_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.14      0.07      0.10        14\n",
            "           2       0.38      0.44      0.41        34\n",
            "           3       0.50      0.05      0.09        42\n",
            "           4       0.24      0.40      0.30        43\n",
            "           5       0.29      0.53      0.38        32\n",
            "           6       0.24      0.15      0.19        39\n",
            "           7       0.15      0.27      0.19        37\n",
            "           8       0.20      0.06      0.10        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.27      0.25      0.22       288\n",
            "weighted avg       0.28      0.25      0.22       288\n",
            "\n",
            "ada_ll_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.14      0.07      0.10        14\n",
            "           2       0.38      0.44      0.41        34\n",
            "           3       0.50      0.05      0.09        42\n",
            "           4       0.24      0.40      0.30        43\n",
            "           5       0.29      0.53      0.38        32\n",
            "           6       0.24      0.15      0.19        39\n",
            "           7       0.15      0.27      0.19        37\n",
            "           8       0.20      0.06      0.10        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.27      0.25      0.22       288\n",
            "weighted avg       0.28      0.25      0.22       288\n",
            "\n",
            "ada_gi_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.05      0.07      0.06        14\n",
            "           2       0.37      0.41      0.39        34\n",
            "           3       0.21      0.21      0.21        42\n",
            "           4       0.19      0.14      0.16        43\n",
            "           5       0.38      0.56      0.46        32\n",
            "           6       0.22      0.15      0.18        39\n",
            "           7       0.17      0.24      0.20        37\n",
            "           8       0.31      0.19      0.24        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.24      0.25      0.24       288\n",
            "weighted avg       0.25      0.25      0.24       288\n",
            "\n",
            "ada_en_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.14      0.07      0.10        14\n",
            "           2       0.38      0.44      0.41        34\n",
            "           3       0.50      0.05      0.09        42\n",
            "           4       0.24      0.40      0.30        43\n",
            "           5       0.29      0.53      0.38        32\n",
            "           6       0.24      0.15      0.19        39\n",
            "           7       0.15      0.27      0.19        37\n",
            "           8       0.20      0.06      0.10        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.27      0.25      0.22       288\n",
            "weighted avg       0.28      0.25      0.22       288\n",
            "\n",
            "ada_ll_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.14      0.07      0.10        14\n",
            "           2       0.38      0.44      0.41        34\n",
            "           3       0.50      0.05      0.09        42\n",
            "           4       0.24      0.40      0.30        43\n",
            "           5       0.29      0.53      0.38        32\n",
            "           6       0.24      0.15      0.19        39\n",
            "           7       0.15      0.27      0.19        37\n",
            "           8       0.20      0.06      0.10        47\n",
            "\n",
            "    accuracy                           0.25       288\n",
            "   macro avg       0.27      0.25      0.22       288\n",
            "weighted avg       0.28      0.25      0.22       288\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melspectogram dataset"
      ],
      "metadata": {
        "id": "9kgiS5ySRygp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_mel_dataset.csv\")"
      ],
      "metadata": {
        "id": "IchpS6m3hl3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying PCA and LDA"
      ],
      "metadata": {
        "id": "EC4_MA5RUN-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 500)\n",
        "lda = LDA(n_components = 7)"
      ],
      "metadata": {
        "id": "NatE2SdVhl3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataset[\"Emotion\"]\n",
        "X = dataset.iloc[:,7:]"
      ],
      "metadata": {
        "id": "EQ3NdL_ghl3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = pca.fit(X)\n",
        "lda = lda.fit(X,Y)\n",
        "X_pca = pca.transform(X)\n",
        "X_lda = lda.transform(X)"
      ],
      "metadata": {
        "id": "sNqVBLs9hl3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_pca,tv_X_pca,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_pca,valid_X_pca,test_Y,valid_Y = train_test_split(tv_X_pca,tv_Y,random_state = 12345,train_size = 0.5)\n",
        "train_X_lda,tv_X_lda,train_Y,tv_Y = train_test_split(X_pca,Y,random_state = 12345,train_size = 0.6)\n",
        "test_X_lda,valid_X_lda,test_Y,valid_Y = train_test_split(tv_X_lda,tv_Y,random_state = 12345,train_size = 0.5)\n"
      ],
      "metadata": {
        "id": "87D2n0Bahl3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kgip = best_para('gini',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kenp = best_para('entropy',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kllp = best_para('log_loss',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kgil = best_para('gini',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "kenl = best_para('entropy',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "klll = best_para('log_loss',train_X_lda,train_Y,valid_X_lda,valid_Y)"
      ],
      "metadata": {
        "id": "dGQnJj4Zhl3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result"
      ],
      "metadata": {
        "id": "KauGnQ1dURu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_gi_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = 7),n_estimators=kgip).fit(train_X_pca,train_Y)\n",
        "ada_en_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = 7),n_estimators=kenp).fit(train_X_pca,train_Y)\n",
        "ada_ll_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = 7),n_estimators=kllp).fit(train_X_pca,train_Y)\n",
        "ada_gi_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = 7),n_estimators=kgil).fit(train_X_lda,train_Y)\n",
        "ada_en_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = 7),n_estimators=kenl).fit(train_X_lda,train_Y)\n",
        "ada_ll_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = 7),n_estimators=klll).fit(train_X_lda,train_Y)\n",
        "\n",
        "\n",
        "clk = [ada_gi_pca,ada_en_pca,ada_ll_pca,ada_gi_lda,ada_en_lda,ada_ll_lda]\n",
        "clk_name = ['ada_gi_pca','ada_en_pca','ada_ll_pca','ada_gi_lda','ada_en_lda','ada_ll_lda']\n",
        "l = len(clk)\n",
        "for i in range(l):\n",
        "  print(clk_name[i])\n",
        "  if i < 3:\n",
        "    test_X = test_X_pca\n",
        "  else:\n",
        "    test_X = test_X_lda\n",
        "  print(classification_report(test_Y,clk[i].predict(test_X)))"
      ],
      "metadata": {
        "id": "4Nk1pLb-hl3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fc43d8-5be4-4744-e413-78b6b8f55066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ada_gi_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.40      0.14      0.21        14\n",
            "           2       0.36      0.53      0.43        34\n",
            "           3       0.34      0.36      0.35        42\n",
            "           4       0.21      0.16      0.18        43\n",
            "           5       0.29      0.38      0.32        32\n",
            "           6       0.24      0.21      0.22        39\n",
            "           7       0.30      0.32      0.31        37\n",
            "           8       0.39      0.34      0.36        47\n",
            "\n",
            "    accuracy                           0.31       288\n",
            "   macro avg       0.32      0.30      0.30       288\n",
            "weighted avg       0.31      0.31      0.30       288\n",
            "\n",
            "ada_en_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.50      0.21      0.30        14\n",
            "           2       0.36      0.65      0.46        34\n",
            "           3       0.19      0.21      0.20        42\n",
            "           4       0.16      0.09      0.12        43\n",
            "           5       0.39      0.41      0.40        32\n",
            "           6       0.28      0.26      0.27        39\n",
            "           7       0.23      0.27      0.25        37\n",
            "           8       0.30      0.23      0.26        47\n",
            "\n",
            "    accuracy                           0.28       288\n",
            "   macro avg       0.30      0.29      0.28       288\n",
            "weighted avg       0.28      0.28      0.27       288\n",
            "\n",
            "ada_ll_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.25      0.07      0.11        14\n",
            "           2       0.37      0.65      0.47        34\n",
            "           3       0.08      0.10      0.09        42\n",
            "           4       0.24      0.14      0.18        43\n",
            "           5       0.28      0.34      0.31        32\n",
            "           6       0.11      0.08      0.09        39\n",
            "           7       0.39      0.41      0.40        37\n",
            "           8       0.26      0.26      0.26        47\n",
            "\n",
            "    accuracy                           0.26       288\n",
            "   macro avg       0.25      0.25      0.24       288\n",
            "weighted avg       0.24      0.26      0.24       288\n",
            "\n",
            "ada_gi_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.40      0.14      0.21        14\n",
            "           2       0.40      0.59      0.48        34\n",
            "           3       0.32      0.29      0.30        42\n",
            "           4       0.26      0.26      0.26        43\n",
            "           5       0.41      0.38      0.39        32\n",
            "           6       0.24      0.15      0.19        39\n",
            "           7       0.26      0.46      0.33        37\n",
            "           8       0.36      0.26      0.30        47\n",
            "\n",
            "    accuracy                           0.32       288\n",
            "   macro avg       0.33      0.31      0.31       288\n",
            "weighted avg       0.32      0.32      0.31       288\n",
            "\n",
            "ada_en_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.40      0.14      0.21        14\n",
            "           2       0.39      0.68      0.49        34\n",
            "           3       0.32      0.26      0.29        42\n",
            "           4       0.14      0.07      0.09        43\n",
            "           5       0.35      0.50      0.41        32\n",
            "           6       0.19      0.15      0.17        39\n",
            "           7       0.21      0.24      0.23        37\n",
            "           8       0.52      0.53      0.53        47\n",
            "\n",
            "    accuracy                           0.33       288\n",
            "   macro avg       0.32      0.32      0.30       288\n",
            "weighted avg       0.31      0.33      0.31       288\n",
            "\n",
            "ada_ll_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.50      0.07      0.12        14\n",
            "           2       0.31      0.47      0.38        34\n",
            "           3       0.29      0.29      0.29        42\n",
            "           4       0.27      0.21      0.24        43\n",
            "           5       0.33      0.41      0.36        32\n",
            "           6       0.38      0.36      0.37        39\n",
            "           7       0.28      0.30      0.29        37\n",
            "           8       0.14      0.13      0.13        47\n",
            "\n",
            "    accuracy                           0.28       288\n",
            "   macro avg       0.31      0.28      0.27       288\n",
            "weighted avg       0.29      0.28      0.28       288\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kgip,dgip,sgip = best_para_with_hp('gini',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kenp,denp,senp = best_para_with_hp('entropy',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kllp,dllp,sllp = best_para_with_hp('log_loss',train_X_pca,train_Y,valid_X_pca,valid_Y)\n",
        "kgil,dgil,sgil = best_para_with_hp('gini',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "kenl,denl,senl = best_para_with_hp('entropy',train_X_lda,train_Y,valid_X_lda,valid_Y)\n",
        "klll,dlll,slll = best_para_with_hp('log_loss',train_X_lda,train_Y,valid_X_lda,valid_Y)"
      ],
      "metadata": {
        "id": "N0u8iEnehl3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result"
      ],
      "metadata": {
        "id": "tWbxEP7cUZH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_gi_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = dgip,min_samples_split=sgip),n_estimators=kgip).fit(train_X_pca,train_Y)\n",
        "ada_en_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = denp,min_samples_split=senp),n_estimators=kenp).fit(train_X_pca,train_Y)\n",
        "ada_ll_pca = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = dllp,min_samples_split=sllp),n_estimators=kllp).fit(train_X_pca,train_Y)\n",
        "ada_gi_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='gini',max_depth = dgil,min_samples_split=sgil),n_estimators=kgil).fit(train_X_lda,train_Y)\n",
        "ada_en_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='entropy',max_depth = denl,min_samples_split=senl),n_estimators=kenl).fit(train_X_lda,train_Y)\n",
        "ada_ll_lda = AdaBoostClassifier(estimator=DecisionTreeClassifier(criterion='log_loss',max_depth = dlll,min_samples_split=slll),n_estimators=klll).fit(train_X_lda,train_Y)\n",
        "\n",
        "\n",
        "clk = [ada_gi_pca,ada_en_pca,ada_ll_pca,ada_gi_lda,ada_en_lda,ada_ll_lda]\n",
        "clk_name = ['ada_gi_pca','ada_en_pca','ada_ll_pca','ada_gi_lda','ada_en_lda','ada_ll_lda']\n",
        "l = len(clk)\n",
        "for i in range(l):\n",
        "  print(clk_name[i])\n",
        "  if i < 3:\n",
        "    test_X = test_X_pca\n",
        "  else:\n",
        "    test_X = test_X_lda\n",
        "  print(classification_report(test_Y,clk[i].predict(test_X)))"
      ],
      "metadata": {
        "id": "MzvbPNPfhl3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5780441-12f4-45ad-92c0-30859885a383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ada_gi_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.35      0.68      0.46        34\n",
            "           3       0.20      0.02      0.04        42\n",
            "           4       0.24      0.26      0.25        43\n",
            "           5       0.44      0.47      0.45        32\n",
            "           6       0.19      0.28      0.23        39\n",
            "           7       0.21      0.38      0.27        37\n",
            "           8       0.31      0.09      0.13        47\n",
            "\n",
            "    accuracy                           0.27       288\n",
            "   macro avg       0.24      0.27      0.23       288\n",
            "weighted avg       0.26      0.27      0.24       288\n",
            "\n",
            "ada_en_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.03      0.07      0.04        14\n",
            "           2       0.36      0.47      0.41        34\n",
            "           3       0.18      0.24      0.20        42\n",
            "           4       0.14      0.07      0.09        43\n",
            "           5       0.12      0.09      0.10        32\n",
            "           6       0.20      0.18      0.19        39\n",
            "           7       0.24      0.27      0.26        37\n",
            "           8       0.28      0.19      0.23        47\n",
            "\n",
            "    accuracy                           0.20       288\n",
            "   macro avg       0.19      0.20      0.19       288\n",
            "weighted avg       0.21      0.20      0.20       288\n",
            "\n",
            "ada_ll_pca\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.03      0.07      0.04        14\n",
            "           2       0.36      0.47      0.41        34\n",
            "           3       0.18      0.24      0.20        42\n",
            "           4       0.14      0.07      0.09        43\n",
            "           5       0.12      0.09      0.10        32\n",
            "           6       0.20      0.18      0.19        39\n",
            "           7       0.24      0.27      0.26        37\n",
            "           8       0.28      0.19      0.23        47\n",
            "\n",
            "    accuracy                           0.20       288\n",
            "   macro avg       0.19      0.20      0.19       288\n",
            "weighted avg       0.21      0.20      0.20       288\n",
            "\n",
            "ada_gi_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.35      0.68      0.46        34\n",
            "           3       0.20      0.02      0.04        42\n",
            "           4       0.24      0.26      0.25        43\n",
            "           5       0.46      0.50      0.48        32\n",
            "           6       0.20      0.28      0.23        39\n",
            "           7       0.21      0.38      0.27        37\n",
            "           8       0.31      0.09      0.13        47\n",
            "\n",
            "    accuracy                           0.28       288\n",
            "   macro avg       0.24      0.28      0.23       288\n",
            "weighted avg       0.26      0.28      0.24       288\n",
            "\n",
            "ada_en_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.03      0.07      0.04        14\n",
            "           2       0.36      0.47      0.41        34\n",
            "           3       0.18      0.24      0.20        42\n",
            "           4       0.14      0.07      0.09        43\n",
            "           5       0.12      0.09      0.10        32\n",
            "           6       0.20      0.18      0.19        39\n",
            "           7       0.24      0.27      0.26        37\n",
            "           8       0.28      0.19      0.23        47\n",
            "\n",
            "    accuracy                           0.20       288\n",
            "   macro avg       0.19      0.20      0.19       288\n",
            "weighted avg       0.21      0.20      0.20       288\n",
            "\n",
            "ada_ll_lda\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.03      0.07      0.04        14\n",
            "           2       0.36      0.47      0.41        34\n",
            "           3       0.18      0.24      0.20        42\n",
            "           4       0.14      0.07      0.09        43\n",
            "           5       0.12      0.09      0.10        32\n",
            "           6       0.20      0.18      0.19        39\n",
            "           7       0.24      0.27      0.26        37\n",
            "           8       0.28      0.19      0.23        47\n",
            "\n",
            "    accuracy                           0.20       288\n",
            "   macro avg       0.19      0.20      0.19       288\n",
            "weighted avg       0.21      0.20      0.20       288\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN"
      ],
      "metadata": {
        "id": "NjgfitEZUgM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FFT dataset"
      ],
      "metadata": {
        "id": "gd9AhArJUuK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_fft_dataset.csv\")"
      ],
      "metadata": {
        "id": "jv5P5NFGYKJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying pca and lda"
      ],
      "metadata": {
        "id": "1e248PH_UxH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 500)\n",
        "lda = LDA(n_components = 7)"
      ],
      "metadata": {
        "id": "R8nIVz33YKJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataset[\"Emotion\"]\n",
        "X = dataset.iloc[:,7:]"
      ],
      "metadata": {
        "id": "zMo2oqjtYKJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cy4BXZKYOom",
        "outputId": "b4401b57-6cc9-45a2-be40-45942ba65e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = pca.fit(X)\n",
        "lda = lda.fit(X,Y)\n",
        "X_pca = pca.transform(X)\n",
        "X_lda = lda.transform(X)\n",
        "X_pca = torch.tensor(X_pca)\n",
        "X_lda = torch.tensor(X_lda)\n",
        "Y = torch.tensor(Y.values - 1)"
      ],
      "metadata": {
        "id": "WujQLzfuYKJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_pca = TensorDataset(X_pca,Y)\n",
        "dataset_lda = TensorDataset(X_lda,Y)\n"
      ],
      "metadata": {
        "id": "AUWR6a52dbgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiuBkllAetNw",
        "outputId": "ceaf5bc0-2da6-42ee-dcdd-1f5721cdc564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1440"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset"
      ],
      "metadata": {
        "id": "6ukpWsGhU1g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pca,test_pca,valid_pca = random_split(dataset_pca,[864,288,288])\n",
        "train_lda,test_lda,valid_lda = random_split(dataset_lda,[864,288,288])\n"
      ],
      "metadata": {
        "id": "NE0Rfvtsd-fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pca_dl = DataLoader(dataset = train_pca,batch_size = 64)\n",
        "test_pca_dl = DataLoader(dataset = test_pca)\n",
        "valid_pca_dl = DataLoader(dataset = valid_pca)\n",
        "train_lda_dl = DataLoader(dataset = train_lda,batch_size = 64)\n",
        "test_lda_dl = DataLoader(dataset = test_lda)\n",
        "valid_lda_dl = DataLoader(dataset = valid_lda)"
      ],
      "metadata": {
        "id": "JhvQO_dufGNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN with sigmoid as activation function and with three hidden layer"
      ],
      "metadata": {
        "id": "pgERMh-BU3y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model_sig(nn.Module):\n",
        "  def __init__(self,input_size,num_classes):\n",
        "    super(model_sig,self).__init__()\n",
        "    self.hid_lay1 = nn.Linear(input_size,500)\n",
        "    self.act_fn1 = nn.Sigmoid()\n",
        "    self.hid_lay2 = nn.Linear(500,300)\n",
        "    self.act_fn2 = nn.Sigmoid()\n",
        "    self.hid_lay3 = nn.Linear(300,8)\n",
        "    self.act_fn3 = nn.Sigmoid()\n",
        "    self.output_layer = nn.Softmax()\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.hid_lay1(x)\n",
        "    x = self.act_fn1(x)\n",
        "    x = self.hid_lay2(x)\n",
        "    x = self.act_fn2(x)\n",
        "    x = self.hid_lay3(x)\n",
        "    x = self.act_fn3(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "    "
      ],
      "metadata": {
        "id": "FQfetZW3gVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzKGHnQrHQiB",
        "outputId": "545fedf4-b4e2-459f-a2a9-1dfdb3c52c02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using PCA dataset"
      ],
      "metadata": {
        "id": "coHa4fpzU_50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C00eie7JFhjD"
      },
      "outputs": [],
      "source": [
        "model = model_sig(500,8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(),lr = 0.0001)"
      ],
      "metadata": {
        "id": "T5R_UdQbha6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "V4F5jkvyjt0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the best model on validation set"
      ],
      "metadata": {
        "id": "xeVkjq0JVVPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(train_pca):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "    if epoch % 25 == 24:\n",
        "        print(f'[{epoch + 1}] Training : loss: {running_loss:0.2f} accuracy : {acc/j}',end='')\n",
        "    with torch.no_grad():\n",
        "      acc = 0.0\n",
        "      j = 0\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(valid_pca):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "      if best_acc < acc/j:\n",
        "        torch.save(model,'/content/drive/MyDrive/PRML_Major_Project/model_pca.pth')\n",
        "        best_acc = acc/j\n",
        "    if epoch % 25 == 24:\n",
        "        print(f' Validation :  loss: {running_loss:0.2f} accuracy : {acc/j}')\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcRs1OShhTAB",
        "outputId": "cc25986f-6de7-4215-a7f5-1c4f9feb7921"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25] Training : loss: 1674.02 accuracy : 0.9710648148148148 Validation :  loss: 592.35 accuracy : 0.3263888888888889\n",
            "[50] Training : loss: 1674.02 accuracy : 0.9629629629629629 Validation :  loss: 592.18 accuracy : 0.3055555555555556\n",
            "[75] Training : loss: 1673.87 accuracy : 0.9398148148148148 Validation :  loss: 591.67 accuracy : 0.3055555555555556\n",
            "[100] Training : loss: 1673.87 accuracy : 0.9317129629629629 Validation :  loss: 591.43 accuracy : 0.2881944444444444\n",
            "[125] Training : loss: 1673.87 accuracy : 0.9305555555555556 Validation :  loss: 590.92 accuracy : 0.2986111111111111\n",
            "[150] Training : loss: 1674.02 accuracy : 0.9652777777777778 Validation :  loss: 593.03 accuracy : 0.2673611111111111\n",
            "[175] Training : loss: 1673.87 accuracy : 0.9490740740740741 Validation :  loss: 592.42 accuracy : 0.2847222222222222\n",
            "[200] Training : loss: 1673.87 accuracy : 0.9363425925925926 Validation :  loss: 592.39 accuracy : 0.3055555555555556\n",
            "[225] Training : loss: 1673.87 accuracy : 0.9317129629629629 Validation :  loss: 592.43 accuracy : 0.2847222222222222\n",
            "[250] Training : loss: 1673.87 accuracy : 0.9351851851851852 Validation :  loss: 591.75 accuracy : 0.2986111111111111\n",
            "[275] Training : loss: 1673.87 accuracy : 0.9884259259259259 Validation :  loss: 591.66 accuracy : 0.2847222222222222\n",
            "[300] Training : loss: 1673.87 accuracy : 0.9872685185185185 Validation :  loss: 592.10 accuracy : 0.2881944444444444\n",
            "[325] Training : loss: 1673.87 accuracy : 0.9780092592592593 Validation :  loss: 592.14 accuracy : 0.3194444444444444\n",
            "[350] Training : loss: 1673.87 accuracy : 0.96875 Validation :  loss: 591.95 accuracy : 0.3090277777777778\n",
            "[375] Training : loss: 1673.87 accuracy : 0.9537037037037037 Validation :  loss: 591.72 accuracy : 0.3229166666666667\n",
            "[400] Training : loss: 1673.88 accuracy : 0.9421296296296297 Validation :  loss: 591.52 accuracy : 0.3125\n",
            "[425] Training : loss: 1673.87 accuracy : 0.9930555555555556 Validation :  loss: 592.11 accuracy : 0.3194444444444444\n",
            "[450] Training : loss: 1673.87 accuracy : 0.9895833333333334 Validation :  loss: 592.11 accuracy : 0.2847222222222222\n",
            "[475] Training : loss: 1673.84 accuracy : 0.9351851851851852 Validation :  loss: 592.18 accuracy : 0.3020833333333333\n",
            "[500] Training : loss: 1673.84 accuracy : 0.9849537037037037 Validation :  loss: 592.48 accuracy : 0.3090277777777778\n",
            "[525] Training : loss: 1673.84 accuracy : 0.9699074074074074 Validation :  loss: 591.62 accuracy : 0.3090277777777778\n",
            "[550] Training : loss: 1673.69 accuracy : 0.9953703703703703 Validation :  loss: 591.37 accuracy : 0.3090277777777778\n",
            "[575] Training : loss: 1673.69 accuracy : 0.9791666666666666 Validation :  loss: 591.57 accuracy : 0.2986111111111111\n",
            "[600] Training : loss: 1673.69 accuracy : 0.9930555555555556 Validation :  loss: 591.74 accuracy : 0.3055555555555556\n",
            "[625] Training : loss: 1673.69 accuracy : 0.9907407407407407 Validation :  loss: 592.09 accuracy : 0.2916666666666667\n",
            "[650] Training : loss: 1673.69 accuracy : 0.9629629629629629 Validation :  loss: 592.22 accuracy : 0.2673611111111111\n",
            "[675] Training : loss: 1673.69 accuracy : 0.9594907407407407 Validation :  loss: 591.68 accuracy : 0.3125\n",
            "[700] Training : loss: 1673.69 accuracy : 0.9942129629629629 Validation :  loss: 592.01 accuracy : 0.3055555555555556\n",
            "[725] Training : loss: 1673.69 accuracy : 0.9942129629629629 Validation :  loss: 591.76 accuracy : 0.3020833333333333\n",
            "[750] Training : loss: 1673.69 accuracy : 0.96875 Validation :  loss: 592.40 accuracy : 0.2951388888888889\n",
            "[775] Training : loss: 1673.69 accuracy : 0.9895833333333334 Validation :  loss: 592.70 accuracy : 0.2951388888888889\n",
            "[800] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 592.63 accuracy : 0.2881944444444444\n",
            "[825] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 592.80 accuracy : 0.28125\n",
            "[850] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 592.89 accuracy : 0.28125\n",
            "[875] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 592.86 accuracy : 0.2847222222222222\n",
            "[900] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 592.92 accuracy : 0.2847222222222222\n",
            "[925] Training : loss: 1673.69 accuracy : 0.9965277777777778 Validation :  loss: 593.18 accuracy : 0.2881944444444444\n",
            "[950] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 593.20 accuracy : 0.2847222222222222\n",
            "[975] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 593.27 accuracy : 0.28125\n",
            "[1000] Training : loss: 1673.69 accuracy : 0.9976851851851852 Validation :  loss: 593.34 accuracy : 0.2881944444444444\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =torch.load('/content/drive/MyDrive/PRML_Major_Project/model_pca.pth')"
      ],
      "metadata": {
        "id": "biC0_nfumzxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "A8njbOakVfan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(test_pca):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "print(f\"accuracy : {acc/j}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFAW4I55m9Ma",
        "outputId": "95b13d67-6674-4aec-b6c9-f3de77b9ce51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.3819444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using LDA dataset"
      ],
      "metadata": {
        "id": "aSum_yr_7XhA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y41ATBXa7iAp"
      },
      "outputs": [],
      "source": [
        "model = model_sig(7,8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(),lr = 0.0001)"
      ],
      "metadata": {
        "id": "PXzbvSGx7iAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "eQmslnts7iAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(train_lda):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "    if epoch % 25 == 24:\n",
        "        print(f'[{epoch + 1}] Training : loss: {running_loss:0.2f} accuracy : {acc/j}',end='')\n",
        "    with torch.no_grad():\n",
        "      acc = 0.0\n",
        "      j = 0\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(valid_lda):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "      if best_acc < acc/j:\n",
        "        torch.save(model,'/content/drive/MyDrive/PRML_Major_Project/model_lda.pth')\n",
        "        best_acc = acc/j\n",
        "    if epoch % 25 == 24:\n",
        "        print(f' Validation :  loss: {running_loss:0.2f} accuracy : {acc/j}')\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4789fba-8078-4262-fdee-7cfae0b1cc65",
        "id": "caSNCY4v7iAq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25] Training : loss: 1677.31 accuracy : 0.9120370370370371 Validation :  loss: 560.36 accuracy : 0.8854166666666666\n",
            "[50] Training : loss: 1676.44 accuracy : 0.9166666666666666 Validation :  loss: 560.31 accuracy : 0.8819444444444444\n",
            "[75] Training : loss: 1675.98 accuracy : 0.9166666666666666 Validation :  loss: 560.27 accuracy : 0.8819444444444444\n",
            "[100] Training : loss: 1675.82 accuracy : 0.9166666666666666 Validation :  loss: 560.28 accuracy : 0.8854166666666666\n",
            "[125] Training : loss: 1675.78 accuracy : 0.9166666666666666 Validation :  loss: 560.27 accuracy : 0.8854166666666666\n",
            "[150] Training : loss: 1675.76 accuracy : 0.9189814814814815 Validation :  loss: 560.27 accuracy : 0.8854166666666666\n",
            "[175] Training : loss: 1675.76 accuracy : 0.9398148148148148 Validation :  loss: 560.28 accuracy : 0.9201388888888888\n",
            "[200] Training : loss: 1675.64 accuracy : 0.9340277777777778 Validation :  loss: 560.47 accuracy : 0.9201388888888888\n",
            "[225] Training : loss: 1675.59 accuracy : 0.9282407407407407 Validation :  loss: 560.48 accuracy : 0.8958333333333334\n",
            "[250] Training : loss: 1675.59 accuracy : 0.9340277777777778 Validation :  loss: 560.49 accuracy : 0.9166666666666666\n",
            "[275] Training : loss: 1675.59 accuracy : 0.9456018518518519 Validation :  loss: 560.48 accuracy : 0.9340277777777778\n",
            "[300] Training : loss: 1675.59 accuracy : 0.9513888888888888 Validation :  loss: 560.48 accuracy : 0.9340277777777778\n",
            "[325] Training : loss: 1675.56 accuracy : 0.9513888888888888 Validation :  loss: 560.44 accuracy : 0.9340277777777778\n",
            "[350] Training : loss: 1675.54 accuracy : 0.9537037037037037 Validation :  loss: 560.46 accuracy : 0.9340277777777778\n",
            "[375] Training : loss: 1675.51 accuracy : 0.9571759259259259 Validation :  loss: 560.49 accuracy : 0.9340277777777778\n",
            "[400] Training : loss: 1675.51 accuracy : 0.9618055555555556 Validation :  loss: 560.57 accuracy : 0.9340277777777778\n",
            "[425] Training : loss: 1675.51 accuracy : 0.9629629629629629 Validation :  loss: 560.58 accuracy : 0.9409722222222222\n",
            "[450] Training : loss: 1675.51 accuracy : 0.9629629629629629 Validation :  loss: 560.58 accuracy : 0.9444444444444444\n",
            "[475] Training : loss: 1675.51 accuracy : 0.9641203703703703 Validation :  loss: 560.58 accuracy : 0.9444444444444444\n",
            "[500] Training : loss: 1675.51 accuracy : 0.9664351851851852 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[525] Training : loss: 1675.51 accuracy : 0.9664351851851852 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[550] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[575] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[600] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[625] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[650] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[675] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[700] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[725] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[750] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[775] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[800] Training : loss: 1675.51 accuracy : 0.9675925925925926 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[825] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[850] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[875] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[900] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[925] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.59 accuracy : 0.9444444444444444\n",
            "[950] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.58 accuracy : 0.9444444444444444\n",
            "[975] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.58 accuracy : 0.9444444444444444\n",
            "[1000] Training : loss: 1675.51 accuracy : 0.9699074074074074 Validation :  loss: 560.58 accuracy : 0.9444444444444444\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =torch.load('/content/drive/MyDrive/PRML_Major_Project/model_lda.pth')"
      ],
      "metadata": {
        "id": "nALIRFGs7iAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result"
      ],
      "metadata": {
        "id": "I9-aYSLNVneX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(test_lda):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "print(f\"accuracy : {acc/j}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf92706-9d41-474d-c4b0-0e6f1ff59b5b",
        "id": "YdBZF8pp7iAq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.9479166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Malspectogram dataset"
      ],
      "metadata": {
        "id": "dZWXUliG7iAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/PRML_Major_Project/audio_mel_dataset.csv\")"
      ],
      "metadata": {
        "id": "hAeK0pFCDwBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying pca and lda"
      ],
      "metadata": {
        "id": "TY_vATw2WaFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 500)\n",
        "lda = LDA(n_components = 7)"
      ],
      "metadata": {
        "id": "o79AWSPaDwBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = dataset[\"Emotion\"]\n",
        "X = dataset.iloc[:,7:]"
      ],
      "metadata": {
        "id": "zJi795uuDwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b00035-3b38-48d1-851a-ecf9c1ccf11b",
        "id": "7_U9tdJfDwBP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = pca.fit(X)\n",
        "lda = lda.fit(X,Y)\n",
        "X_pca = pca.transform(X)\n",
        "X_lda = lda.transform(X)\n",
        "X_pca = torch.tensor(X_pca)\n",
        "X_lda = torch.tensor(X_lda)\n",
        "Y = torch.tensor(Y.values - 1)"
      ],
      "metadata": {
        "id": "iJ6KjCsQDwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_pca = TensorDataset(X_pca,Y)\n",
        "dataset_lda = TensorDataset(X_lda,Y)\n"
      ],
      "metadata": {
        "id": "c8paohxRDwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ec0dd0-ac66-41bd-a666-fd6648ce375d",
        "id": "tQZVof7lDwBP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1440"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pca,test_pca,valid_pca = random_split(dataset_pca,[864,288,288])\n",
        "train_lda,test_lda,valid_lda = random_split(dataset_lda,[864,288,288])\n"
      ],
      "metadata": {
        "id": "3P-Q7Kj9DwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pca_dl = DataLoader(dataset = train_pca,batch_size = 64)\n",
        "test_pca_dl = DataLoader(dataset = test_pca)\n",
        "valid_pca_dl = DataLoader(dataset = valid_pca)\n",
        "train_lda_dl = DataLoader(dataset = train_lda,batch_size = 64)\n",
        "test_lda_dl = DataLoader(dataset = test_lda)\n",
        "valid_lda_dl = DataLoader(dataset = valid_lda)"
      ],
      "metadata": {
        "id": "JhyLY3aeDwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc01578-39ad-487f-875b-d651e439ad1f",
        "id": "BSL8mk0yDwBQ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using pca dataset\n"
      ],
      "metadata": {
        "id": "Ovwgd9AfWe5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS2c1HoIDwBQ"
      },
      "outputs": [],
      "source": [
        "model = model_sig(500,8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(),lr = 0.0001)"
      ],
      "metadata": {
        "id": "483OBY93DwBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "hRC5TSbhDwBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(train_pca):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "    if epoch % 25 == 24:\n",
        "        print(f'[{epoch + 1}] Training : loss: {running_loss:0.2f} accuracy : {acc/j}',end='')\n",
        "    with torch.no_grad():\n",
        "      acc = 0.0\n",
        "      j = 0\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(valid_pca):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "      if best_acc < acc/j:\n",
        "        torch.save(model,'/content/drive/MyDrive/PRML_Major_Project/model2_pca.pth')\n",
        "        best_acc = acc/j\n",
        "    if epoch % 25 == 24:\n",
        "        print(f' Validation :  loss: {running_loss:0.2f} accuracy : {acc/j}')\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1e30b2-9b8a-4240-d85a-c0e272afe576",
        "id": "54jM7kU2DwBQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25] Training : loss: 1712.20 accuracy : 0.6875 Validation :  loss: 591.05 accuracy : 0.2916666666666667\n",
            "[50] Training : loss: 1703.53 accuracy : 0.7349537037037037 Validation :  loss: 591.77 accuracy : 0.2847222222222222\n",
            "[75] Training : loss: 1702.37 accuracy : 0.7395833333333334 Validation :  loss: 590.61 accuracy : 0.3055555555555556\n",
            "[100] Training : loss: 1701.08 accuracy : 0.7442129629629629 Validation :  loss: 592.61 accuracy : 0.2777777777777778\n",
            "[125] Training : loss: 1700.95 accuracy : 0.7430555555555556 Validation :  loss: 592.62 accuracy : 0.28125\n",
            "[150] Training : loss: 1701.00 accuracy : 0.7453703703703703 Validation :  loss: 592.02 accuracy : 0.28125\n",
            "[175] Training : loss: 1701.04 accuracy : 0.7465277777777778 Validation :  loss: 591.76 accuracy : 0.2951388888888889\n",
            "[200] Training : loss: 1700.42 accuracy : 0.7488425925925926 Validation :  loss: 591.95 accuracy : 0.2951388888888889\n",
            "[225] Training : loss: 1700.39 accuracy : 0.7465277777777778 Validation :  loss: 592.22 accuracy : 0.2777777777777778\n",
            "[250] Training : loss: 1699.89 accuracy : 0.7465277777777778 Validation :  loss: 591.92 accuracy : 0.28125\n",
            "[275] Training : loss: 1700.03 accuracy : 0.7488425925925926 Validation :  loss: 592.70 accuracy : 0.2673611111111111\n",
            "[300] Training : loss: 1699.85 accuracy : 0.75 Validation :  loss: 591.61 accuracy : 0.2916666666666667\n",
            "[325] Training : loss: 1700.08 accuracy : 0.75 Validation :  loss: 590.89 accuracy : 0.3020833333333333\n",
            "[350] Training : loss: 1699.37 accuracy : 0.7511574074074074 Validation :  loss: 591.39 accuracy : 0.3055555555555556\n",
            "[375] Training : loss: 1700.25 accuracy : 0.7511574074074074 Validation :  loss: 591.42 accuracy : 0.2916666666666667\n",
            "[400] Training : loss: 1698.73 accuracy : 0.7615740740740741 Validation :  loss: 592.04 accuracy : 0.2673611111111111\n",
            "[425] Training : loss: 1697.23 accuracy : 0.7662037037037037 Validation :  loss: 592.26 accuracy : 0.2638888888888889\n",
            "[450] Training : loss: 1697.00 accuracy : 0.7719907407407407 Validation :  loss: 592.10 accuracy : 0.2743055555555556\n",
            "[475] Training : loss: 1698.57 accuracy : 0.7696759259259259 Validation :  loss: 592.46 accuracy : 0.2743055555555556\n",
            "[500] Training : loss: 1696.67 accuracy : 0.7754629629629629 Validation :  loss: 591.25 accuracy : 0.2916666666666667\n",
            "[525] Training : loss: 1696.18 accuracy : 0.7754629629629629 Validation :  loss: 594.77 accuracy : 0.2222222222222222\n",
            "[550] Training : loss: 1695.92 accuracy : 0.7789351851851852 Validation :  loss: 592.08 accuracy : 0.2777777777777778\n",
            "[575] Training : loss: 1696.57 accuracy : 0.7731481481481481 Validation :  loss: 592.88 accuracy : 0.25\n",
            "[600] Training : loss: 1695.80 accuracy : 0.78125 Validation :  loss: 592.60 accuracy : 0.2534722222222222\n",
            "[625] Training : loss: 1695.71 accuracy : 0.7870370370370371 Validation :  loss: 592.83 accuracy : 0.2569444444444444\n",
            "[650] Training : loss: 1695.65 accuracy : 0.7881944444444444 Validation :  loss: 592.25 accuracy : 0.2638888888888889\n",
            "[675] Training : loss: 1696.84 accuracy : 0.7800925925925926 Validation :  loss: 591.77 accuracy : 0.28125\n",
            "[700] Training : loss: 1695.38 accuracy : 0.7905092592592593 Validation :  loss: 591.89 accuracy : 0.2847222222222222\n",
            "[725] Training : loss: 1695.22 accuracy : 0.7974537037037037 Validation :  loss: 591.59 accuracy : 0.2847222222222222\n",
            "[750] Training : loss: 1695.23 accuracy : 0.7986111111111112 Validation :  loss: 591.57 accuracy : 0.28125\n",
            "[775] Training : loss: 1698.60 accuracy : 0.7719907407407407 Validation :  loss: 591.85 accuracy : 0.2881944444444444\n",
            "[800] Training : loss: 1695.04 accuracy : 0.7997685185185185 Validation :  loss: 591.80 accuracy : 0.2743055555555556\n",
            "[825] Training : loss: 1697.27 accuracy : 0.7893518518518519 Validation :  loss: 591.69 accuracy : 0.2986111111111111\n",
            "[850] Training : loss: 1694.71 accuracy : 0.7997685185185185 Validation :  loss: 591.64 accuracy : 0.2743055555555556\n",
            "[875] Training : loss: 1695.35 accuracy : 0.7916666666666666 Validation :  loss: 591.65 accuracy : 0.2847222222222222\n",
            "[900] Training : loss: 1695.37 accuracy : 0.7962962962962963 Validation :  loss: 591.36 accuracy : 0.2881944444444444\n",
            "[925] Training : loss: 1695.50 accuracy : 0.7951388888888888 Validation :  loss: 591.43 accuracy : 0.2881944444444444\n",
            "[950] Training : loss: 1694.87 accuracy : 0.7974537037037037 Validation :  loss: 592.74 accuracy : 0.28125\n",
            "[975] Training : loss: 1695.35 accuracy : 0.8020833333333334 Validation :  loss: 592.56 accuracy : 0.2777777777777778\n",
            "[1000] Training : loss: 1694.90 accuracy : 0.8043981481481481 Validation :  loss: 591.42 accuracy : 0.2881944444444444\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =torch.load('/content/drive/MyDrive/PRML_Major_Project/model2_pca.pth')"
      ],
      "metadata": {
        "id": "5xreHMyrDwBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "eGyx7B_TWiEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(test_pca):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "print(f\"accuracy : {acc/j}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6549e429-7e75-4faa-f13b-cff0f9f0f3e3",
        "id": "0BewwkZUDwBR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.2951388888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using lda dataset\n"
      ],
      "metadata": {
        "id": "2-QsCUVRDwBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzhMn3p4DwBS"
      },
      "outputs": [],
      "source": [
        "model = model_sig(7,8).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(),lr = 0.0001)"
      ],
      "metadata": {
        "id": "6OTFsiJEDwBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "zRY9tNAVDwBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(train_lda):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "    if epoch % 25 == 24:\n",
        "        print(f'[{epoch + 1}] Training : loss: {running_loss:0.2f} accuracy : {acc/j}',end='')\n",
        "    with torch.no_grad():\n",
        "      acc = 0.0\n",
        "      j = 0\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(valid_lda):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        labels = labels.to(device = device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "      if best_acc < acc/j:\n",
        "        torch.save(model,'/content/drive/MyDrive/PRML_Major_Project/model2_lda.pth')\n",
        "        best_acc = acc/j\n",
        "    if epoch % 25 == 24:\n",
        "        print(f' Validation :  loss: {running_loss:0.2f} accuracy : {acc/j}')\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726c55c0-c6f9-4989-e39f-954bfbd2c2e3",
        "id": "ejdqNYYeDwBS"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25] Training : loss: 1665.34 accuracy : 0.9942129629629629 Validation :  loss: 555.36 accuracy : 0.9895833333333334\n",
            "[50] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.44 accuracy : 0.9861111111111112\n",
            "[75] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.40 accuracy : 0.9895833333333334\n",
            "[100] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.41 accuracy : 0.9895833333333334\n",
            "[125] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.41 accuracy : 0.9895833333333334\n",
            "[150] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.41 accuracy : 0.9895833333333334\n",
            "[175] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.42 accuracy : 0.9895833333333334\n",
            "[200] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.42 accuracy : 0.9895833333333334\n",
            "[225] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.42 accuracy : 0.9895833333333334\n",
            "[250] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.42 accuracy : 0.9895833333333334\n",
            "[275] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[300] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[325] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[350] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[375] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[400] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[425] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[450] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[475] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[500] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[525] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[550] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[575] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[600] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[625] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[650] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[675] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[700] Training : loss: 1665.28 accuracy : 0.9942129629629629 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[725] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[750] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[775] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[800] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[825] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[850] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[875] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[900] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[925] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[950] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.43 accuracy : 0.9895833333333334\n",
            "[975] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.44 accuracy : 0.9895833333333334\n",
            "[1000] Training : loss: 1665.28 accuracy : 0.9953703703703703 Validation :  loss: 555.44 accuracy : 0.9895833333333334\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =torch.load('/content/drive/MyDrive/PRML_Major_Project/model2_lda.pth')"
      ],
      "metadata": {
        "id": "DjeVUJ4-RZJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "OgE6ld5LWnjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    acc = 0.0\n",
        "    j = 0\n",
        "    for i, data in enumerate(test_lda):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device = device).float()\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _,pred = torch.max(outputs,0)\n",
        "        pred = pred.cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "        acc += np.sum(pred == labels)\n",
        "        j += 1\n",
        "print(f\"accuracy : {acc/j}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50bdae2-caa2-4f65-e69b-389518123d9d",
        "id": "RRcTeDB_DwBT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.9930555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-92a50441495b>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.output_layer(x)\n"
          ]
        }
      ]
    }
  ]
}